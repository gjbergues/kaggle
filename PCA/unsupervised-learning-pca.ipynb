{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-08T16:45:51.652339Z","iopub.execute_input":"2022-02-08T16:45:51.653781Z","iopub.status.idle":"2022-02-08T16:45:51.682060Z","shell.execute_reply.started":"2022-02-08T16:45:51.653675Z","shell.execute_reply":"2022-02-08T16:45:51.681441Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Importing the Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:48:03.942511Z","iopub.execute_input":"2022-02-08T16:48:03.943349Z","iopub.status.idle":"2022-02-08T16:48:04.967095Z","shell.execute_reply.started":"2022-02-08T16:48:03.943303Z","shell.execute_reply":"2022-02-08T16:48:04.966158Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Importing the Dataset\ntry:\n    data = pd.read_csv(\"../input/wholesale-customer-data/Wholesale customers data.csv\")\n    data.drop(labels=(['Channel','Region']),axis=1,inplace=True)\n    print('Wholesale customers has {} samples with {} features each'.format(*data.shape))\nexcept:\n    print('Sorry! Dataset could not be loaded.')","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:48:44.592187Z","iopub.execute_input":"2022-02-08T16:48:44.592512Z","iopub.status.idle":"2022-02-08T16:48:44.629036Z","shell.execute_reply.started":"2022-02-08T16:48:44.592484Z","shell.execute_reply":"2022-02-08T16:48:44.627871Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Check Data\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:54:38.756449Z","iopub.execute_input":"2022-02-08T16:54:38.756783Z","iopub.status.idle":"2022-02-08T16:54:38.768608Z","shell.execute_reply.started":"2022-02-08T16:54:38.756752Z","shell.execute_reply":"2022-02-08T16:54:38.767663Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Display a brief description of the overall dataset\ndata.describe()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:54:41.557415Z","iopub.execute_input":"2022-02-08T16:54:41.557704Z","iopub.status.idle":"2022-02-08T16:54:41.591406Z","shell.execute_reply.started":"2022-02-08T16:54:41.557670Z","shell.execute_reply":"2022-02-08T16:54:41.590456Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Display complete information of the data frame\ndata.info()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:54:44.524207Z","iopub.execute_input":"2022-02-08T16:54:44.525500Z","iopub.status.idle":"2022-02-08T16:54:44.541209Z","shell.execute_reply.started":"2022-02-08T16:54:44.525431Z","shell.execute_reply":"2022-02-08T16:54:44.540159Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Select three indices of your choice you wish to sample from the dataset\nindices = [22,154,398]\n\n# Create a DataFrame of the chosen samples\nsamples = pd.DataFrame(data.loc[indices], columns=data.keys()).reset_index(drop=True)\nprint(\"Chosen samples of wholesale customers dataset:\")\ndisplay(samples)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:54:57.189254Z","iopub.execute_input":"2022-02-08T16:54:57.190238Z","iopub.status.idle":"2022-02-08T16:54:57.204959Z","shell.execute_reply.started":"2022-02-08T16:54:57.190096Z","shell.execute_reply":"2022-02-08T16:54:57.204144Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# look at percentile ranks\n#pcts = 100. * data.rank(axis=0, pct=True).iloc[indices].round(decimals=3)\npcts = 100. * data.rank(axis=0, pct=True).iloc[indices].round(decimals=3)\n# visualize percentiles with heatmap\n\nsns.heatmap(pcts, annot=True, vmin=1, vmax=99, fmt='.1f', cmap='YlGnBu')\nplt.title('Percentile ranks of\\nsamples\\' category spending')\nplt.xticks(rotation=45, ha='center');","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:55:01.092305Z","iopub.execute_input":"2022-02-08T16:55:01.092729Z","iopub.status.idle":"2022-02-08T16:55:01.377714Z","shell.execute_reply.started":"2022-02-08T16:55:01.092690Z","shell.execute_reply":"2022-02-08T16:55:01.376633Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"**Samples:**\n\n*      0: This customer ranks above the 90th percentile for annual spending amounts in Fresh, Frozen, and the Delicatessen categories. These features along with above average spending for detergents_paper could lead us to believe this customer is a market. Markets generally put an emphasis on having a large variety of fresh foods available and often contain a delicatessen or deli.\n*      1: On the opposite side of the spectrum, this customer ranks in the bottom 10th percentile across all product categories. It's highest ranking category is 'Fresh' which might suggest it is a small cafe or similar.\n*      2: Our last customer spends a lot in the Fresh and Frozen categories but moreso in the latter. I would suspect this is a wholesale retailer because of the focus on Fresh and Frozen foods.\n\nOne interesting thought to consider is if one (or more) of the six product categories is actually relevant for understanding customer purchasing. That is to say, is it possible to determine whether customers purchasing some amount of one category of products will necessarily purchase some proportional amount of another category of products? We can make this determination quite easily by training a supervised regression learner on a subset of the data with one feature removed, and then score how well that model can predict the removed feature.","metadata":{}},{"cell_type":"code","source":"# Import libraries for Decision Tree Regressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\n\n# Remove column Milk\nnew_data = data.drop('Milk',axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:57:26.956617Z","iopub.execute_input":"2022-02-08T16:57:26.957426Z","iopub.status.idle":"2022-02-08T16:57:26.964855Z","shell.execute_reply.started":"2022-02-08T16:57:26.957352Z","shell.execute_reply":"2022-02-08T16:57:26.963428Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Split the data into training and testing sets(0.25) using the given feature as the target\n# Set a random state.\nX_train, X_test, y_train, y_test = train_test_split(new_data, data['Milk'], test_size=0.25, random_state=1)\n\n# Create a decision tree regressor and fit it to the training set\nregressor =  DecisionTreeRegressor(random_state=1)\nregressor.fit(X_train, y_train)\n\n# Report the score of the prediction using the testing set\nscore = regressor.score(X_test, y_test)\nprint(score)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:57:32.386968Z","iopub.execute_input":"2022-02-08T16:57:32.387755Z","iopub.status.idle":"2022-02-08T16:57:32.405025Z","shell.execute_reply.started":"2022-02-08T16:57:32.387691Z","shell.execute_reply":"2022-02-08T16:57:32.403986Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"As you can see, we attempted to predict Milk using the other features in the dataset and the score ended up being 0.515. At this initial stage we might say that this feature is somewhat difficult to predict because the score is around the halfway point of possible scores. Remember that R^2 goes from 0 to 1. This might indicate that it could be an important feature to consider.\n\n# **Visualize Feature Distributions**","metadata":{}},{"cell_type":"code","source":"pd.plotting.scatter_matrix(data, alpha=0.3,figsize=(15,8),diagonal='kde' )\nplt.tight_layout() # To avoid overlapping of plots","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:58:21.967728Z","iopub.execute_input":"2022-02-08T16:58:21.968054Z","iopub.status.idle":"2022-02-08T16:58:24.344973Z","shell.execute_reply.started":"2022-02-08T16:58:21.968014Z","shell.execute_reply":"2022-02-08T16:58:24.343845Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"Milk showed some signs of correlation for about half of the features it was compared to which aligns with our earlier prediction. The pair of features with the highest correlation are Detergents_Paper and Grocery which intuitively makes sense as many people shop for both when they go \"grocery shopping.\" One other visible point to note is how many of the points are around 0 for features compared to Delicatessen. The data for all of these features are right-skewed with many points hovering at the origin or near it and long tails.\n\n","metadata":{}},{"cell_type":"markdown","source":"# **Data Preprocessing**\n\nNow we will start to preprocess the data to create a better representation of customers by performing a scaling on the data and detecting (and optionally removing) outliers. Preprocessing data is often times a critical step in assuring that results we obtain from your analysis are significant and meaningful.\n","metadata":{}},{"cell_type":"markdown","source":"# **Implementation: Feature Scaling**\n\nFeature scaling is a method used to standardize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing step","metadata":{}},{"cell_type":"code","source":"# Scale the data using the natural logarithm\nlog_data = np.log(data.copy())\n\n# Scale the sample data using the natural logarithm\nlog_samples = np.log(samples)\n\n# Produce a scatter matrix for each pair of newly-transformed features\npd.plotting.scatter_matrix(log_data, alpha=0.5, figsize=(14,8),diagonal='kde')\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:58:54.844144Z","iopub.execute_input":"2022-02-08T16:58:54.844495Z","iopub.status.idle":"2022-02-08T16:58:57.288428Z","shell.execute_reply.started":"2022-02-08T16:58:54.844461Z","shell.execute_reply":"2022-02-08T16:58:57.287571Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"# **Observation:**\n\nAfter applying a natural logarithm scaling to the data, the distribution of each feature appears much more normal. For any pairs of features you may have identified earlier as being correlated, observe here whether that correlation is still present (and whether it is now stronger or weaker than before).","metadata":{}},{"cell_type":"code","source":"# Let's compare the original sample data to the log-transformed sample data\nprint(\"Original chosen samples of wholesale customers dataset:\")\ndisplay(samples)\n\n# Display the log-transformed sample data\nprint(\"Log-transformed samples of wholesale customers dataset:\")\ndisplay(log_samples)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:59:14.021646Z","iopub.execute_input":"2022-02-08T16:59:14.021957Z","iopub.status.idle":"2022-02-08T16:59:14.046358Z","shell.execute_reply.started":"2022-02-08T16:59:14.021927Z","shell.execute_reply":"2022-02-08T16:59:14.045668Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"# **Implementation: Outlier Detection**\n\nDetecting outliers in the data is extremely important in the data preprocessing step of any analysis. The presence of outliers can often skew results which take into consideration these data points. There are many \"rules of thumb\" for what constitutes an outlier in a dataset. Here, we will use Tukey's Method for identfying outliers: An outlier step is calculated as 1.5 times the interquartile range (IQR). A data point with a feature that is beyond an outlier step outside of the IQR for that feature is considered abnormal","metadata":{}},{"cell_type":"code","source":"# For each feature find the data points with extreme high or low values\nfor feature in log_data.keys():\n\n    # Calculate Q1 (25th percentile of the data) for the given feature\n    Q1 = np.percentile(log_data, 25)\n\n    # Calculate Q3 (75th percentile of the data) for the given feature\n    Q3 = np.percentile(log_data, 75)\n\n    # Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n    step = (Q3 - Q1) * 1.5\n    \n# Display the outliers\n    print(\"Data points considered outliers for the feature '{}':\".format(feature))\n    display(log_data[~((log_data[feature] >= Q1 - step) & (log_data[feature] <= Q3 + step))])\n    \n    # Select the indices for data points you wish to remove\noutliers  = [66, 75, 338, 142, 154, 289]\n\n# Remove the outliers, if any were specified\ngood_data = log_data.drop(log_data.index[outliers]).reset_index(drop = True)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:59:32.428276Z","iopub.execute_input":"2022-02-08T16:59:32.429149Z","iopub.status.idle":"2022-02-08T16:59:32.506449Z","shell.execute_reply.started":"2022-02-08T16:59:32.429102Z","shell.execute_reply":"2022-02-08T16:59:32.505734Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"There were a handful of specific rows containing outliers in multiple features based on our definition of an outlier. I chose to remove these rows because having a row show up as multiple outliers can add to our confidence that it is truly an outlier","metadata":{}},{"cell_type":"markdown","source":"# **Feature Transformation**\n\nIn this section we will use principal component analysis (PCA) to draw conclusions about the underlying structure of the wholesale customer data. Since using PCA on a dataset calculates the dimensions which best maximize variance, we will find which compound combinations of features best describe customers.","metadata":{}},{"cell_type":"markdown","source":"# **Implementation: PCA**\n\nNow that the data has been scaled to a more normal distribution and has had any necessary outliers removed, we can go ahead and apply PCA to the good_data to discover which dimensions about the data best maximize the variance of features involved. In addition to finding these dimensions, PCA will also report the explained variance ratio of each dimension — how much variance within the data is explained by that dimension alone. Note that a component (dimension) from PCA can be considered a new \"feature\" of the space, however it is a composition of the original features present in the data.","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\n# Apply PCA by fitting the good data with the same number of dimensions as features\npca = PCA(n_components=6)\npca.fit(good_data)\n\n# Transform log_samples using the PCA fit above\npca_samples = pca.transform(log_samples)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T17:00:02.145138Z","iopub.execute_input":"2022-02-08T17:00:02.145442Z","iopub.status.idle":"2022-02-08T17:00:02.156861Z","shell.execute_reply.started":"2022-02-08T17:00:02.145413Z","shell.execute_reply":"2022-02-08T17:00:02.155820Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"print(pca.components_)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T17:00:04.168773Z","iopub.execute_input":"2022-02-08T17:00:04.169315Z","iopub.status.idle":"2022-02-08T17:00:04.176263Z","shell.execute_reply.started":"2022-02-08T17:00:04.169259Z","shell.execute_reply":"2022-02-08T17:00:04.175626Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"print(pca.explained_variance_)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T16:53:07.657631Z","iopub.execute_input":"2022-02-08T16:53:07.657905Z","iopub.status.idle":"2022-02-08T16:53:07.663706Z","shell.execute_reply.started":"2022-02-08T16:53:07.657877Z","shell.execute_reply":"2022-02-08T16:53:07.662645Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"pca_samples","metadata":{"execution":{"iopub.status.busy":"2022-02-08T17:00:07.123147Z","iopub.execute_input":"2022-02-08T17:00:07.123625Z","iopub.status.idle":"2022-02-08T17:00:07.129747Z","shell.execute_reply.started":"2022-02-08T17:00:07.123586Z","shell.execute_reply":"2022-02-08T17:00:07.128891Z"},"trusted":true},"execution_count":34,"outputs":[]}]}