{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-12-03T18:05:43.490448Z",
     "iopub.status.busy": "2021-12-03T18:05:43.489793Z",
     "iopub.status.idle": "2021-12-03T18:05:44.455045Z",
     "shell.execute_reply": "2021-12-03T18:05:44.454026Z",
     "shell.execute_reply.started": "2021-12-03T18:05:43.490325Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Ignore warnings for clear view\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-03T18:05:47.355669Z",
     "iopub.status.busy": "2021-12-03T18:05:47.355408Z",
     "iopub.status.idle": "2021-12-03T18:06:04.615453Z",
     "shell.execute_reply": "2021-12-03T18:06:04.613714Z",
     "shell.execute_reply.started": "2021-12-03T18:05:47.355640Z"
    }
   },
   "outputs": [],
   "source": [
    "#Read DataSet from Kaggle\n",
    "df = pd.read_csv('/kaggle/input/tabular-playground-series-dec-2021/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-03T18:06:25.613750Z",
     "iopub.status.busy": "2021-12-03T18:06:25.613040Z",
     "iopub.status.idle": "2021-12-03T18:06:25.661202Z",
     "shell.execute_reply": "2021-12-03T18:06:25.660161Z",
     "shell.execute_reply.started": "2021-12-03T18:06:25.613680Z"
    }
   },
   "outputs": [],
   "source": [
    "# First look of the Data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-03T18:06:29.285080Z",
     "iopub.status.busy": "2021-12-03T18:06:29.284465Z",
     "iopub.status.idle": "2021-12-03T18:06:29.289294Z",
     "shell.execute_reply": "2021-12-03T18:06:29.288476Z",
     "shell.execute_reply.started": "2021-12-03T18:06:29.285027Z"
    }
   },
   "outputs": [],
   "source": [
    "#Drop the first column 'Id' (serial numbers). Not useful in the prediction.\n",
    "df = df.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-03T18:06:31.853176Z",
     "iopub.status.busy": "2021-12-03T18:06:31.852530Z",
     "iopub.status.idle": "2021-12-03T18:06:31.860604Z",
     "shell.execute_reply": "2021-12-03T18:06:31.859187Z",
     "shell.execute_reply.started": "2021-12-03T18:06:31.853122Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data frame size\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-03T18:06:33.800687Z",
     "iopub.status.busy": "2021-12-03T18:06:33.800368Z",
     "iopub.status.idle": "2021-12-03T18:06:33.808689Z",
     "shell.execute_reply": "2021-12-03T18:06:33.807692Z",
     "shell.execute_reply.started": "2021-12-03T18:06:33.800654Z"
    }
   },
   "outputs": [],
   "source": [
    "# Datatypes of the attributes\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-03T18:06:37.123759Z",
     "iopub.status.busy": "2021-12-03T18:06:37.123450Z",
     "iopub.status.idle": "2021-12-03T18:06:42.447489Z",
     "shell.execute_reply": "2021-12-03T18:06:42.446385Z",
     "shell.execute_reply.started": "2021-12-03T18:06:37.123727Z"
    }
   },
   "outputs": [],
   "source": [
    "# Statistical description\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(df.describe())\n",
    "\n",
    "# Learning :\n",
    "# No attribute is missing as count is 4000000 for all attributes. Hence, all rows can be used\n",
    "# Negative values are present in some columns. Hence, some tests such as chi-sq can't be used.\n",
    "# Attributes Soil_Type7 and Soil_Type15 can be removed as they are constant.\n",
    "# Scales are not the same for all. Hence, rescaling and standardization may be necessary for some algos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-03T18:06:50.197023Z",
     "iopub.status.busy": "2021-12-03T18:06:50.196446Z",
     "iopub.status.idle": "2021-12-03T18:06:53.750238Z",
     "shell.execute_reply": "2021-12-03T18:06:53.748101Z",
     "shell.execute_reply.started": "2021-12-03T18:06:50.196987Z"
    }
   },
   "outputs": [],
   "source": [
    "# Skewness of the distribution\n",
    "# A measure of the asymmetry of the probability distribution of a real-valued random variable about its mean.\n",
    "# Values close to 0 show less skew\n",
    "print(df.skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-03T18:06:56.730675Z",
     "iopub.status.busy": "2021-12-03T18:06:56.730386Z",
     "iopub.status.idle": "2021-12-03T18:06:56.804600Z",
     "shell.execute_reply": "2021-12-03T18:06:56.803614Z",
     "shell.execute_reply.started": "2021-12-03T18:06:56.730647Z"
    }
   },
   "outputs": [],
   "source": [
    "# Number of instances belonging to each class\n",
    "df.groupby('Cover_Type').size()\n",
    "\n",
    "# Re-balancing is necessary, the classes are in different quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-03T18:07:00.953969Z",
     "iopub.status.busy": "2021-12-03T18:07:00.952984Z",
     "iopub.status.idle": "2021-12-03T18:07:02.446431Z",
     "shell.execute_reply": "2021-12-03T18:07:02.445724Z",
     "shell.execute_reply.started": "2021-12-03T18:07:00.953914Z"
    }
   },
   "outputs": [],
   "source": [
    "# Correlation tells relation between two attributes.\n",
    "# Correlation requires continous data. Hence, ignore Wilderness_Area and Soil_Type as they are binary.\n",
    "\n",
    "# Number of features considered\n",
    "size = 10 \n",
    "\n",
    "# Create dataframe with the 10 first attributes\n",
    "data = df.iloc[:, :size] \n",
    "\n",
    "# Get the names of all the columns\n",
    "cols = data.columns \n",
    "\n",
    "# Calculates pearson coefficient for all combinations\n",
    "data_corr = data.corr()\n",
    "\n",
    "# Set the threshold to select only only highly correlated attributes\n",
    "threshold = 0.5\n",
    "\n",
    "# List of pairs along with correlation above threshold\n",
    "corr_list = []\n",
    "\n",
    "# Search for the highly correlated pairs\n",
    "for i in range(0, size): #for 'size' features\n",
    "    for j in range(i+1, size): # Avoid repetition\n",
    "        if (data_corr.iloc[i, j] >= threshold and data_corr.iloc[i, j] < 1) or (data_corr.iloc[i, j] < 0 and data_corr.iloc[i, j] <= -threshold):\n",
    "            corr_list.append([data_corr.iloc[i, j],i, j]) # Store correlation and columns index\n",
    "\n",
    "# Sort to show higher ones first            \n",
    "s_corr_list = sorted(corr_list,key=lambda x: -abs(x[0]))\n",
    "\n",
    "# Print correlations and column names\n",
    "for v,i,j in s_corr_list:\n",
    "    print (\"%s and %s = %.2f\" % (cols[i],cols[j],v))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T19:50:57.736021Z",
     "iopub.status.busy": "2021-12-02T19:50:57.735469Z",
     "iopub.status.idle": "2021-12-02T19:50:57.755555Z",
     "shell.execute_reply": "2021-12-02T19:50:57.754724Z",
     "shell.execute_reply.started": "2021-12-02T19:50:57.735983Z"
    }
   },
   "source": [
    "Result = Not Strong correlation is observed between the pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-03T18:07:03.006942Z",
     "iopub.status.busy": "2021-12-03T18:07:03.006237Z",
     "iopub.status.idle": "2021-12-03T18:15:20.923625Z",
     "shell.execute_reply": "2021-12-03T18:15:20.922186Z",
     "shell.execute_reply.started": "2021-12-03T18:07:03.006903Z"
    }
   },
   "outputs": [],
   "source": [
    "#names of all the attributes \n",
    "cols = df.columns\n",
    "\n",
    "#number of attributes (exclude target)\n",
    "size = len(cols)-1\n",
    "\n",
    "#x-axis has target attribute to distinguish between classes\n",
    "x = cols[size]\n",
    "\n",
    "#y-axis shows values of an attribute\n",
    "y = cols[0:size]\n",
    "\n",
    "#Plot violin for all attributes\n",
    "for i in range(0,size):\n",
    "    sns.violinplot(data=df,x=x,y=y[i])  \n",
    "    plt.show()\n",
    "\n",
    "# Elevation \n",
    "# Aspect \n",
    "# Horizontal distance to road\n",
    "# hydrology \n",
    "# Hillshade 9am and 12pm \n",
    "# Hillshade 3pm \n",
    "# Wilderness_Area3 \n",
    "# Soil_Type, 1,5,8,9,12,14,18-22, 25-30 and 35-40 offer class distinction as values are not present for many classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-03T18:15:20.925706Z",
     "iopub.status.busy": "2021-12-03T18:15:20.925444Z"
    }
   },
   "outputs": [],
   "source": [
    "# Group one-hot encoded variables of a category into one single variable\n",
    "\n",
    "#names of all the columns\n",
    "cols = df.columns\n",
    "\n",
    "#number of rows=r , number of columns=c\n",
    "r,c = df.shape\n",
    "\n",
    "#Create a new dataframe with r rows, one column for each encoded category, and target in the end\n",
    "data = pd.DataFrame(index=np.arange(0, r),columns=['Wilderness_Area', 'Soil_Type', 'Cover_Type'])\n",
    "\n",
    "#Make an entry in 'data' for each r as category_id, target value\n",
    "for i in range(0,r):\n",
    "    w = 0;\n",
    "    s = 0;\n",
    "    # Category1 range\n",
    "    for j in range(10, 14):\n",
    "        if (df.iloc[i, j] == 1):\n",
    "            w = j-9  #category class\n",
    "            break\n",
    "    # Category2 range        \n",
    "    for k in range(14, 54):\n",
    "        if (df.iloc[i,k] == 1):\n",
    "            s = k-13 # Category class\n",
    "            break\n",
    "    #Make an entry in 'data' for each r as category_id, target value        \n",
    "    data.iloc[i] = [w, s, df.iloc[i, c-1]]\n",
    "\n",
    "#Plot for Category1    \n",
    "sns.countplot(x=\"Wilderness_Area\", hue=\"Cover_Type\", data=data)\n",
    "plt.show()\n",
    "#Plot for Category2\n",
    "plt.rc(\"figure\", figsize=(25, 10))\n",
    "sns.countplot(x=\"Soil_Type\", hue=\"Cover_Type\", data=data)\n",
    "plt.show()\n",
    "\n",
    "#(right-click and open the image in a new window for larger size)\n",
    "#WildernessArea_4 has a lot of presence for cover_type 4. Good class distinction\n",
    "#WildernessArea_3 has not much class distinction\n",
    "#SoilType 1-6,10-14,17, 22-23, 29-33,35,38-40 offer lot of class distinction as counts for some are very high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removal list initialize\n",
    "rem = []\n",
    "\n",
    "#Add constant columns as they don't help in prediction process\n",
    "for c in df.columns:\n",
    "    if df[c].std() == 0: #standard deviation is zero\n",
    "        rem.append(c)\n",
    "\n",
    "#drop the columns        \n",
    "df.drop(rem,axis=1,inplace=True)\n",
    "\n",
    "print(rem)\n",
    "\n",
    "#Following columns are dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-01T19:22:37.111939Z",
     "iopub.status.idle": "2021-12-01T19:22:37.112452Z",
     "shell.execute_reply": "2021-12-01T19:22:37.112269Z",
     "shell.execute_reply.started": "2021-12-01T19:22:37.112222Z"
    }
   },
   "outputs": [],
   "source": [
    "#get the number of rows and columns\n",
    "r, c = df.shape\n",
    "\n",
    "#get the list of columns\n",
    "cols = df.columns\n",
    "#create an array which has indexes of columns\n",
    "i_cols = []\n",
    "for i in range(0,c-1):\n",
    "    i_cols.append(i)\n",
    "#array of importance rank of all features  \n",
    "ranks = []\n",
    "\n",
    "#Extract only the values\n",
    "array = df.values\n",
    "\n",
    "#Y is the target column, X has the rest\n",
    "X = array[:,0:(c-1)]\n",
    "Y = array[:,(c-1)]\n",
    "\n",
    "#Validation chunk size\n",
    "val_size = 0.1\n",
    "\n",
    "#Use a common seed in all experiments so that same chunk is used for validation\n",
    "seed = 0\n",
    "\n",
    "#Split the data into chunks\n",
    "from sklearn import cross_validation\n",
    "X_train, X_val, Y_train, Y_val = cross_validation.train_test_split(X, Y, test_size=val_size, random_state=seed)\n",
    "\n",
    "#Import libraries for data transformations\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#All features\n",
    "X_all = []\n",
    "#Additionally we will make a list of subsets\n",
    "X_all_add =[]\n",
    "\n",
    "#columns to be dropped\n",
    "rem = []\n",
    "#indexes of columns to be dropped\n",
    "i_rem = []\n",
    "\n",
    "#List of combinations\n",
    "comb = []\n",
    "comb.append(\"All+1.0\")\n",
    "\n",
    "#Add this version of X to the list \n",
    "X_all.append(['Orig','All', X_train,X_val,1.0,cols[:c-1],rem,ranks,i_cols,i_rem])\n",
    "\n",
    "#point where categorical data begins\n",
    "size=10\n",
    "\n",
    "#Standardized\n",
    "#Apply transform only for non-categorical data\n",
    "X_temp = StandardScaler().fit_transform(X_train[:,0:size])\n",
    "X_val_temp = StandardScaler().fit_transform(X_val[:,0:size])\n",
    "#Concatenate non-categorical data and categorical\n",
    "X_con = numpy.concatenate((X_temp,X_train[:,size:]),axis=1)\n",
    "X_val_con = numpy.concatenate((X_val_temp,X_val[:,size:]),axis=1)\n",
    "#Add this version of X to the list \n",
    "X_all.append(['StdSca','All', X_con,X_val_con,1.0,cols,rem,ranks,i_cols,i_rem])\n",
    "\n",
    "#MinMax\n",
    "#Apply transform only for non-categorical data\n",
    "X_temp = MinMaxScaler().fit_transform(X_train[:,0:size])\n",
    "X_val_temp = MinMaxScaler().fit_transform(X_val[:,0:size])\n",
    "#Concatenate non-categorical data and categorical\n",
    "X_con = numpy.concatenate((X_temp,X_train[:,size:]),axis=1)\n",
    "X_val_con = numpy.concatenate((X_val_temp,X_val[:,size:]),axis=1)\n",
    "#Add this version of X to the list \n",
    "X_all.append(['MinMax', 'All', X_con,X_val_con,1.0,cols,rem,ranks,i_cols,i_rem])\n",
    "\n",
    "#Normalize\n",
    "#Apply transform only for non-categorical data\n",
    "X_temp = Normalizer().fit_transform(X_train[:,0:size])\n",
    "X_val_temp = Normalizer().fit_transform(X_val[:,0:size])\n",
    "#Concatenate non-categorical data and categorical\n",
    "X_con = numpy.concatenate((X_temp,X_train[:,size:]),axis=1)\n",
    "X_val_con = numpy.concatenate((X_val_temp,X_val[:,size:]),axis=1)\n",
    "#Add this version of X to the list \n",
    "X_all.append(['Norm', 'All', X_con,X_val_con,1.0,cols,rem,ranks,i_cols,i_rem])\n",
    "\n",
    "#Impute\n",
    "#Imputer is not used as no data is missing\n",
    "\n",
    "#List of transformations\n",
    "trans_list = []\n",
    "\n",
    "for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n",
    "    trans_list.append(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-01T19:22:37.113501Z",
     "iopub.status.idle": "2021-12-01T19:22:37.114097Z",
     "shell.execute_reply": "2021-12-01T19:22:37.113922Z",
     "shell.execute_reply.started": "2021-12-01T19:22:37.113903Z"
    }
   },
   "outputs": [],
   "source": [
    "#Select top 75%,50%,25%\n",
    "ratio_list = [0.75,0.50,0.25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-01T19:22:37.115205Z",
     "iopub.status.idle": "2021-12-01T19:22:37.115709Z",
     "shell.execute_reply": "2021-12-01T19:22:37.115553Z",
     "shell.execute_reply.started": "2021-12-01T19:22:37.115533Z"
    }
   },
   "outputs": [],
   "source": [
    "#List of feature selection models\n",
    "feat = []\n",
    "\n",
    "#List of names of feature selection models\n",
    "feat_list =[]\n",
    "\n",
    "#Import the libraries\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#Add ExtraTreeClassifiers to the list\n",
    "n = 'ExTree'\n",
    "feat_list.append(n)\n",
    "for val in ratio_list:\n",
    "    comb.append(\"%s+%s\" % (n,val))\n",
    "    feat.append([n,val,ExtraTreesClassifier(n_estimators=c-1,max_features=val,n_jobs=-1,random_state=seed)])      \n",
    "\n",
    "#Add GradientBoostingClassifiers to the list \n",
    "n = 'GraBst'\n",
    "feat_list.append(n)\n",
    "for val in ratio_list:\n",
    "    comb.append(\"%s+%s\" % (n,val))\n",
    "    feat.append([n,val,GradientBoostingClassifier(n_estimators=c-1,max_features=val,random_state=seed)])   \n",
    "\n",
    "#Add RandomForestClassifiers to the list \n",
    "n = 'RndFst'\n",
    "feat_list.append(n)\n",
    "for val in ratio_list:\n",
    "    comb.append(\"%s+%s\" % (n,val))\n",
    "    feat.append([n,val,RandomForestClassifier(n_estimators=c-1,max_features=val,n_jobs=-1,random_state=seed)])   \n",
    "\n",
    "#Add XGBClassifier to the list \n",
    "n = 'XGB'\n",
    "feat_list.append(n)\n",
    "for val in ratio_list:\n",
    "    comb.append(\"%s+%s\" % (n,val))\n",
    "    feat.append([n,val,XGBClassifier(n_estimators=c-1,seed=seed)])   \n",
    "        \n",
    "#For all transformations of X\n",
    "for trans,s, X, X_val, d, cols, rem, ra, i_cols, i_rem in X_all:\n",
    "    #For all feature selection models\n",
    "    for name,v, model in feat:\n",
    "        #Train the model against Y\n",
    "        model.fit(X,Y_train)\n",
    "        #Combine importance and index of the column in the array joined\n",
    "        joined = []\n",
    "        for i, pred in enumerate(list(model.feature_importances_)):\n",
    "            joined.append([i,cols[i],pred])\n",
    "        #Sort in descending order    \n",
    "        joined_sorted = sorted(joined, key=lambda x: -x[2])\n",
    "        #Starting point of the columns to be dropped\n",
    "        rem_start = int((v*(c-1)))\n",
    "        #List of names of columns selected\n",
    "        cols_list = []\n",
    "        #Indexes of columns selected\n",
    "        i_cols_list = []\n",
    "        #Ranking of all the columns\n",
    "        rank_list =[]\n",
    "        #List of columns not selected\n",
    "        rem_list = []\n",
    "        #Indexes of columns not selected\n",
    "        i_rem_list = []\n",
    "        #Split the array. Store selected columns in cols_list and removed in rem_list\n",
    "        for j, (i, col, x) in enumerate(list(joined_sorted)):\n",
    "            #Store the rank\n",
    "            rank_list.append([i,j])\n",
    "            #Store selected columns in cols_list and indexes in i_cols_list\n",
    "            if(j < rem_start):\n",
    "                cols_list.append(col)\n",
    "                i_cols_list.append(i)\n",
    "            #Store not selected columns in rem_list and indexes in i_rem_list    \n",
    "            else:\n",
    "                rem_list.append(col)\n",
    "                i_rem_list.append(i)    \n",
    "        #Sort the rank_list and store only the ranks. Drop the index \n",
    "        #Append model name, array, columns selected and columns to be removed to the additional list        \n",
    "        X_all_add.append([trans,name,X,X_val,v,cols_list,rem_list,[x[1] for x in sorted(rank_list,key=lambda x:x[0])],i_cols_list,i_rem_list])    \n",
    "\n",
    "#Set figure size\n",
    "plt.rc(\"figure\", figsize=(25, 10))\n",
    "\n",
    "#Plot a graph for different feature selectors        \n",
    "for f_name in feat_list:\n",
    "    #Array to store the list of combinations\n",
    "    leg=[]\n",
    "    fig, ax = plt.subplots()\n",
    "    #Plot each combination\n",
    "    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n",
    "        if(name==f_name):\n",
    "            plt.plot(rank_list)\n",
    "            leg.append(trans+\"+\"+name+\"+%s\"% v)\n",
    "    #Set the tick names to names of columns\n",
    "    ax.set_xticks(range(c-1))\n",
    "    ax.set_xticklabels(cols[:c-1],rotation='vertical')\n",
    "    #Display the plot\n",
    "    plt.legend(leg,loc='best')    \n",
    "    #Plot the rankings of all the features for all combinations\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-01T19:22:50.983195Z",
     "iopub.status.busy": "2021-12-01T19:22:50.982741Z",
     "iopub.status.idle": "2021-12-01T19:22:51.411342Z",
     "shell.execute_reply": "2021-12-01T19:22:51.4097Z",
     "shell.execute_reply.started": "2021-12-01T19:22:50.983159Z"
    }
   },
   "outputs": [],
   "source": [
    "#List of feature selection models\n",
    "feat = []\n",
    "\n",
    "#List of names of feature selection models\n",
    "feat_list =[]\n",
    "\n",
    "#Libraries for feature selection\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Add RFE to the list \n",
    "model = LogisticRegression(random_state=seed,n_jobs=-1)\n",
    "n = 'RFE'\n",
    "feat_list.append(n)\n",
    "for val in ratio_list:\n",
    "    comb.append(\"%s+%s\" % (n,val))\n",
    "    feat.append([n,val,RFE(model,val*(c-1))])   \n",
    "        \n",
    "#For all transformations of X\n",
    "for trans,s, X, X_val, d, cols, rem, ra, i_cols, i_rem in X_all:\n",
    "    #For all feature selection models\n",
    "    for name,v, model in feat:\n",
    "        #Train the model against Y\n",
    "        model.fit(X,Y_train)\n",
    "        #Combine importance and index of the column in the array joined\n",
    "        joined = []\n",
    "        for i, pred in enumerate(list(model.ranking_)):\n",
    "            joined.append([i,cols[i],pred])\n",
    "        #Sort in ascending order    \n",
    "        joined_sorted = sorted(joined, key=lambda x: x[2])\n",
    "        #Starting point of the columns to be dropped\n",
    "        rem_start = int((v*(c-1)))\n",
    "        #List of names of columns selected\n",
    "        cols_list = []\n",
    "        #Indexes of columns selected\n",
    "        i_cols_list = []\n",
    "        #Ranking of all the columns\n",
    "        rank_list =[]\n",
    "        #List of columns not selected\n",
    "        rem_list = []\n",
    "        #Indexes of columns not selected\n",
    "        i_rem_list = []\n",
    "        #Split the array. Store selected columns in cols_list and removed in rem_list\n",
    "        for i, col, j in joined_sorted:\n",
    "            #Store the rank\n",
    "            rank_list.append([i,j-1])\n",
    "            #Store selected columns in cols_list and indexes in i_cols_list\n",
    "            if((j-1) < rem_start):\n",
    "                cols_list.append(col)\n",
    "                i_cols_list.append(i)\n",
    "            #Store not selected columns in rem_list and indexes in i_rem_list    \n",
    "            else:\n",
    "                rem_list.append(col)\n",
    "                i_rem_list.append(i)    \n",
    "        #Sort the rank_list and store only the ranks. Drop the index \n",
    "        #Append model name, array, columns selected and columns to be removed to the additional list        \n",
    "        X_all_add.append([trans,name,X,X_val,v,cols_list,rem_list,[x[1] for x in sorted(rank_list,key=lambda x:x[0])],i_cols_list,i_rem_list])    \n",
    "\n",
    "#Set figure size\n",
    "plt.rc(\"figure\", figsize=(25, 10))\n",
    "\n",
    "#Plot a graph for different feature selectors        \n",
    "for f_name in feat_list:\n",
    "    #Array to store the list of combinations\n",
    "    leg=[]\n",
    "    fig, ax = plt.subplots()\n",
    "    #Plot each combination\n",
    "    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n",
    "        if(name==f_name):\n",
    "            plt.plot(rank_list)\n",
    "            leg.append(trans+\"+\"+name+\"+%s\"% v)\n",
    "    #Set the tick names to names of columns\n",
    "    ax.set_xticks(range(c-1))\n",
    "    ax.set_xticklabels(cols[:c-1],rotation='vertical')\n",
    "    #Display the plot\n",
    "    plt.legend(leg,loc='best')    \n",
    "    #Plot the rankings of all the features for all combinations\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-01T19:23:30.499757Z",
     "iopub.status.busy": "2021-12-01T19:23:30.4995Z",
     "iopub.status.idle": "2021-12-01T19:23:30.525964Z",
     "shell.execute_reply": "2021-12-01T19:23:30.524702Z",
     "shell.execute_reply.started": "2021-12-01T19:23:30.49973Z"
    }
   },
   "outputs": [],
   "source": [
    "#List of feature selection models\n",
    "feat = []\n",
    "\n",
    "#List of names of feature selection models\n",
    "feat_list =[]\n",
    "\n",
    "#Libraries for SelectPercentile    \n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import f_classif        \n",
    "\n",
    "n = 'SelK'\n",
    "feat_list.append(n)\n",
    "for val in ratio_list:\n",
    "    comb.append(\"%s+%s\" % (n,val))\n",
    "    feat.append([n,val,SelectPercentile(score_func=f_classif,percentile=val*100)])   \n",
    "\n",
    "#For all transformations of X\n",
    "for trans,s, X, X_val, d, cols, rem, ra, i_cols, i_rem in X_all:\n",
    "    #For all feature selection models\n",
    "    for name,v, model in feat:\n",
    "        #Train the model against Y\n",
    "        model.fit(X,Y_train)\n",
    "        #Combine importance and index of the column in the array joined\n",
    "        joined = []\n",
    "        for i, pred in enumerate(list(model.scores_)):\n",
    "            joined.append([i,cols[i],pred])\n",
    "        #Sort in descending order    \n",
    "        joined_sorted = sorted(joined, key=lambda x: -x[2])\n",
    "        #Starting point of the columns to be dropped\n",
    "        rem_start = int((v*(c-1)))\n",
    "        #List of names of columns selected\n",
    "        cols_list = []\n",
    "        #Indexes of columns selected\n",
    "        i_cols_list = []\n",
    "        #Ranking of all the columns\n",
    "        rank_list =[]\n",
    "        #List of columns not selected\n",
    "        rem_list = []\n",
    "        #Indexes of columns not selected\n",
    "        i_rem_list = []\n",
    "        #Split the array. Store selected columns in cols_list and removed in rem_list\n",
    "        for j, (i, col, x) in enumerate(list(joined_sorted)):\n",
    "            #Store the rank\n",
    "            rank_list.append([i,j])\n",
    "            #Store selected columns in cols_list and indexes in i_cols_list\n",
    "            if(j < rem_start):\n",
    "                cols_list.append(col)\n",
    "                i_cols_list.append(i)\n",
    "            #Store not selected columns in rem_list and indexes in i_rem_list    \n",
    "            else:\n",
    "                rem_list.append(col)\n",
    "                i_rem_list.append(i)    \n",
    "        #Sort the rank_list and store only the ranks. Drop the index \n",
    "        #Append model name, array, columns selected and columns to be removed to the additional list        \n",
    "        X_all_add.append([trans,name,X,X_val,v,cols_list,rem_list,[x[1] for x in sorted(rank_list,key=lambda x:x[0])],i_cols_list,i_rem_list])    \n",
    "\n",
    "\n",
    "#Set figure size\n",
    "plt.rc(\"figure\", figsize=(25, 10))\n",
    "\n",
    "#Plot a graph for different feature selectors        \n",
    "for f_name in feat_list:\n",
    "    #Array to store the list of combinations\n",
    "    leg=[]\n",
    "    fig, ax = plt.subplots()\n",
    "    #Plot each combination\n",
    "    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n",
    "        if(name==f_name):\n",
    "            plt.plot(rank_list)\n",
    "            leg.append(trans+\"+\"+name+\"+%s\"% v)\n",
    "    #Set the tick names to names of columns\n",
    "    ax.set_xticks(range(c-1))\n",
    "    ax.set_xticklabels(cols[:c-1],rotation='vertical')\n",
    "    #Display the plot\n",
    "    plt.legend(leg,loc='best')    \n",
    "    #Plot the rankings of all the features for all combinations\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-01T19:23:52.281734Z",
     "iopub.status.busy": "2021-12-01T19:23:52.280983Z",
     "iopub.status.idle": "2021-12-01T19:23:52.298594Z",
     "shell.execute_reply": "2021-12-01T19:23:52.297367Z",
     "shell.execute_reply.started": "2021-12-01T19:23:52.281697Z"
    }
   },
   "outputs": [],
   "source": [
    "rank_df = pandas.DataFrame(data=[x[7] for x in X_all_add],columns=cols[:c-1])\n",
    "med = rank_df.median()\n",
    "print(med)\n",
    "#Write medians to output file for exploratory study on ML algorithms\n",
    "with open(\"median.csv\", \"w\") as subfile:\n",
    "       subfile.write(\"Column,Median\\n\")\n",
    "       subfile.write(med.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
