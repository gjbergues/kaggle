{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook shows how we can simplify the feature selection procedure utilising an open Python library called [Feature-engine](https://feature-engine.readthedocs.io/en/latest/index.html).\n\nThe latest version of [Feature-engine](https://feature-engine.readthedocs.io/en/latest/index.html) features several methods to [select features](https://feature-engine.readthedocs.io/en/latest/selection/index.html) that are not available in other libraries at the moment.\n\n[Feature-engine](https://feature-engine.readthedocs.io/en/latest/index.html) classes preserve Scikit-learn functionality with the methods **fit** and **transform** to first learn the parameters from the data, and then transform the data utilizing those parameters.\n\nBy selecting features, we can build simpler, faster and more interpretable machine learning models.\n \n\n## Table of Contents\n\n- Remove constant and quasi-constant features\n- Remove duplicated features\n- Remove correlated features with a brute force approach or selecting features smartly\n- Select important features by feature shuffling\n- Select features based on a univariate model performance\n- Select features recursively\n- Build an entire machine learning pipeline followed by a machine learning model\n\n## Additional Resources\n\n- [Feature Selection for Machine Learning](https://www.udemy.com/course/feature-selection-for-machine-learning/?referralCode=186501DF5D93F48C4F71) - Online Course\n- [Feature Selection for Machine Learning: A Comprehensive Overview](https://trainindata.medium.com/feature-selection-for-machine-learning-a-comprehensive-overview-bd571db5dd2d) - Article\n- [Feature Selection with Feature-engine](https://feature-engine.readthedocs.io/en/latest/selection/index.html) - Package Documentation\n","metadata":{}},{"cell_type":"code","source":"# let's install Feature-engine\n!pip install feature-engine","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:32:03.678750Z","iopub.execute_input":"2022-02-15T19:32:03.679354Z","iopub.status.idle":"2022-02-15T19:32:14.364963Z","shell.execute_reply.started":"2022-02-15T19:32:03.679308Z","shell.execute_reply":"2022-02-15T19:32:14.363640Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n\n# import selection classes from Feature-engine\nfrom feature_engine.selection import (\n    DropDuplicateFeatures,\n    DropConstantFeatures,\n    DropDuplicateFeatures,\n    DropCorrelatedFeatures,\n    SmartCorrelatedSelection,\n    SelectByShuffling,\n    SelectBySingleFeaturePerformance,\n    RecursiveFeatureElimination,\n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-15T19:32:39.049794Z","iopub.execute_input":"2022-02-15T19:32:39.050198Z","iopub.status.idle":"2022-02-15T19:32:40.374213Z","shell.execute_reply.started":"2022-02-15T19:32:39.050163Z","shell.execute_reply":"2022-02-15T19:32:40.373087Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# load the Santander customer satisfaction dataset\ndata = pd.read_csv('/kaggle/input/santander-customer-satisfaction/train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:32:46.255647Z","iopub.execute_input":"2022-02-15T19:32:46.256028Z","iopub.status.idle":"2022-02-15T19:32:49.271720Z","shell.execute_reply.started":"2022-02-15T19:32:46.255995Z","shell.execute_reply":"2022-02-15T19:32:49.270525Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Check columns and data\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:33:19.739237Z","iopub.execute_input":"2022-02-15T19:33:19.739644Z","iopub.status.idle":"2022-02-15T19:33:19.783887Z","shell.execute_reply.started":"2022-02-15T19:33:19.739594Z","shell.execute_reply":"2022-02-15T19:33:19.782855Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"X = data.drop(labels=['ID','TARGET'], axis=1)\ny = data['TARGET']\n\n# separate dataset into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\nX_train.shape, X_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:36:02.174060Z","iopub.execute_input":"2022-02-15T19:36:02.174480Z","iopub.status.idle":"2022-02-15T19:36:02.488012Z","shell.execute_reply.started":"2022-02-15T19:36:02.174447Z","shell.execute_reply":"2022-02-15T19:36:02.486891Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# check missing data \n[x for x in X_train.columns if X_train[x].isnull().sum() > 0]","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:37:09.529012Z","iopub.execute_input":"2022-02-15T19:37:09.529672Z","iopub.status.idle":"2022-02-15T19:37:09.623623Z","shell.execute_reply.started":"2022-02-15T19:37:09.529630Z","shell.execute_reply":"2022-02-15T19:37:09.622585Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Remove constant features\n\nConstant features are those which contain only 1 value for all the observations.","metadata":{}},{"cell_type":"code","source":"# with tol=1 we tell the transformer to remove constant features\nconstant = DropConstantFeatures(tol=1)\n\n# finds the constant features on the train set\nconstant.fit(X_train)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:39:03.497393Z","iopub.execute_input":"2022-02-15T19:39:03.497889Z","iopub.status.idle":"2022-02-15T19:39:03.832727Z","shell.execute_reply.started":"2022-02-15T19:39:03.497846Z","shell.execute_reply":"2022-02-15T19:39:03.831408Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# the constant features can be found in the attribute\n# features_to_drop_\nlen(constant.features_to_drop_)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:39:17.890129Z","iopub.execute_input":"2022-02-15T19:39:17.890540Z","iopub.status.idle":"2022-02-15T19:39:17.896533Z","shell.execute_reply.started":"2022-02-15T19:39:17.890496Z","shell.execute_reply":"2022-02-15T19:39:17.895515Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# show the names of the first 3 constant features\nconstant.features_to_drop_[0:3]","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:39:27.479305Z","iopub.execute_input":"2022-02-15T19:39:27.479684Z","iopub.status.idle":"2022-02-15T19:39:27.486188Z","shell.execute_reply.started":"2022-02-15T19:39:27.479652Z","shell.execute_reply":"2022-02-15T19:39:27.485266Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# check if the feature is constant (it has only 1 value in all the observations)\nX_train['ind_var2_0'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:40:23.548158Z","iopub.execute_input":"2022-02-15T19:40:23.548568Z","iopub.status.idle":"2022-02-15T19:40:23.556501Z","shell.execute_reply.started":"2022-02-15T19:40:23.548533Z","shell.execute_reply":"2022-02-15T19:40:23.555418Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# remove constant features - transform method\nprint('Number of variables before removing constant: ', X_train.shape[1])\n\nX_train = constant.transform(X_train)\nX_test = constant.transform(X_test)\n\nprint('Number of variables after removing constant: ', X_train.shape[1])","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:40:50.272496Z","iopub.execute_input":"2022-02-15T19:40:50.273106Z","iopub.status.idle":"2022-02-15T19:40:50.443929Z","shell.execute_reply.started":"2022-02-15T19:40:50.273066Z","shell.execute_reply":"2022-02-15T19:40:50.442943Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Remove Quasi-constant features\n\nQuasi-constant features are those that show the same value in most of the observations in the dataset.","metadata":{}},{"cell_type":"code","source":"# with tol=0.998 we tell the transformer that we want to remove\n# all features that show the same value in more than 99.8% of the\n# observations in the dataset\nquasi_constant = DropConstantFeatures(tol=0.998)\n\n# find quasi-constant features in the train set\nquasi_constant.fit(X_train)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:41:10.338086Z","iopub.execute_input":"2022-02-15T19:41:10.338513Z","iopub.status.idle":"2022-02-15T19:41:11.039089Z","shell.execute_reply.started":"2022-02-15T19:41:10.338473Z","shell.execute_reply":"2022-02-15T19:41:11.037737Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# the constant features can be found in the attribute\n# features_to_drop_\nlen(constant.features_to_drop_)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:41:16.384991Z","iopub.execute_input":"2022-02-15T19:41:16.385585Z","iopub.status.idle":"2022-02-15T19:41:16.390917Z","shell.execute_reply.started":"2022-02-15T19:41:16.385526Z","shell.execute_reply":"2022-02-15T19:41:16.389919Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# show the names of the first 3 constant features\nquasi_constant.features_to_drop_[0:3]","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:41:19.299181Z","iopub.execute_input":"2022-02-15T19:41:19.299795Z","iopub.status.idle":"2022-02-15T19:41:19.306440Z","shell.execute_reply.started":"2022-02-15T19:41:19.299738Z","shell.execute_reply":"2022-02-15T19:41:19.305217Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# we can evaluate the percentage of observations that show\n# each value\nX_train['imp_op_var40_efect_ult1'].value_counts() / len(X_train)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:41:26.209340Z","iopub.execute_input":"2022-02-15T19:41:26.210065Z","iopub.status.idle":"2022-02-15T19:41:26.219706Z","shell.execute_reply.started":"2022-02-15T19:41:26.210024Z","shell.execute_reply":"2022-02-15T19:41:26.218621Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"We can see that most of the observations show the value 0.0. A few of them take a different value.","metadata":{}},{"cell_type":"code","source":"# remove quasi-constant features - transform method\nprint('Number of variables before removing quasi-constant: ', X_train.shape[1])\n\nX_train = quasi_constant.transform(X_train)\nX_test = quasi_constant.transform(X_test)\n\nprint('Number of variables after removing quasi-constant: ', X_train.shape[1])","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:42:14.988555Z","iopub.execute_input":"2022-02-15T19:42:14.990467Z","iopub.status.idle":"2022-02-15T19:42:15.203150Z","shell.execute_reply.started":"2022-02-15T19:42:14.990380Z","shell.execute_reply":"2022-02-15T19:42:15.201800Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Remove duplicated features\n\nDrop features that are identical.","metadata":{}},{"cell_type":"code","source":"duplicates = DropDuplicateFeatures()\n\n# find duplicated features in the train set\nduplicates.fit(X_train)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:42:53.707530Z","iopub.execute_input":"2022-02-15T19:42:53.708384Z","iopub.status.idle":"2022-02-15T19:42:55.256820Z","shell.execute_reply.started":"2022-02-15T19:42:53.708336Z","shell.execute_reply":"2022-02-15T19:42:55.255808Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# the groups or identical variables can be seen in the \n# attribute duplicated_feature_sets\nduplicates.duplicated_feature_sets_","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:42:58.672408Z","iopub.execute_input":"2022-02-15T19:42:58.672781Z","iopub.status.idle":"2022-02-15T19:42:58.678808Z","shell.execute_reply.started":"2022-02-15T19:42:58.672748Z","shell.execute_reply":"2022-02-15T19:42:58.678037Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# we can go ahead and check that these variables are indeed identical\n# take for example the first pair in the above cell\nX_train['ind_var26'].equals(X_train['ind_var26_0'])","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:43:04.967326Z","iopub.execute_input":"2022-02-15T19:43:04.968017Z","iopub.status.idle":"2022-02-15T19:43:04.975360Z","shell.execute_reply.started":"2022-02-15T19:43:04.967977Z","shell.execute_reply":"2022-02-15T19:43:04.974058Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# inspect the values of some observations\nX_train[['ind_var26','ind_var26_0']].head()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:43:11.767225Z","iopub.execute_input":"2022-02-15T19:43:11.767630Z","iopub.status.idle":"2022-02-15T19:43:11.780931Z","shell.execute_reply.started":"2022-02-15T19:43:11.767591Z","shell.execute_reply":"2022-02-15T19:43:11.780129Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# in the attribute features_to_drop_ we find the variables\n# from the groups of duplicates that will be dropped\n\n# the transformer only leaves 1 variable per group and removes\n# the rest.\nduplicates.features_to_drop_","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:43:27.234999Z","iopub.execute_input":"2022-02-15T19:43:27.235568Z","iopub.status.idle":"2022-02-15T19:43:27.241861Z","shell.execute_reply.started":"2022-02-15T19:43:27.235529Z","shell.execute_reply":"2022-02-15T19:43:27.240675Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# remove duplicates - transform method\nprint('Number of variables before removing duplicates: ', X_train.shape[1])\n\nX_train = duplicates.transform(X_train)\nX_test = duplicates.transform(X_test)\n\nprint('Number of variables after removing duplicates: ', X_train.shape[1])","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:43:38.097582Z","iopub.execute_input":"2022-02-15T19:43:38.097977Z","iopub.status.idle":"2022-02-15T19:43:38.188066Z","shell.execute_reply.started":"2022-02-15T19:43:38.097942Z","shell.execute_reply":"2022-02-15T19:43:38.187142Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"## Drop Correlated features\n\n","metadata":{}},{"cell_type":"code","source":"# if variables is set to None, the transformer will examine all variables\n# we can choose the correlation method to use (pearson, spearman or kendal)\n# and the correlation threshold\ncorrelated = DropCorrelatedFeatures(variables=None, method='pearson', threshold=0.8)\n\n# find correlated variables in the train set\ncorrelated.fit(X_train)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:44:34.621921Z","iopub.execute_input":"2022-02-15T19:44:34.622540Z","iopub.status.idle":"2022-02-15T19:44:38.512560Z","shell.execute_reply.started":"2022-02-15T19:44:34.622501Z","shell.execute_reply":"2022-02-15T19:44:38.511360Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# in the attribute correlated_feature_sets_ we find the \n# variables that are correlated with each other\n\n# note that several variables can be correlated with each other\ncorrelated.correlated_feature_sets_","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:44:44.350716Z","iopub.execute_input":"2022-02-15T19:44:44.351501Z","iopub.status.idle":"2022-02-15T19:44:44.363126Z","shell.execute_reply.started":"2022-02-15T19:44:44.351448Z","shell.execute_reply":"2022-02-15T19:44:44.361981Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# let's plot a correlation heat map for the following group:\n# (the first one in the sets above)\ncorrmat = X_train[[\n    'imp_op_var39_comer_ult1',\n    'imp_op_var39_comer_ult3',\n    'imp_op_var41_comer_ult1',\n    'imp_op_var41_comer_ult3']].corr(method='pearson')\n\n# we can make a heatmap with the package seaborn\n# and customise the colours of searborn's heatmap\ncmap = sns.diverging_palette(220, 20, as_cmap=True)\n\n# some more parameters for the figure\nfig, ax = plt.subplots()\nfig.set_size_inches(5,5)\n\n# and now plot the correlation matrix\nsns.heatmap(corrmat, cmap=cmap)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:45:17.431277Z","iopub.execute_input":"2022-02-15T19:45:17.432062Z","iopub.status.idle":"2022-02-15T19:45:17.928875Z","shell.execute_reply.started":"2022-02-15T19:45:17.432007Z","shell.execute_reply":"2022-02-15T19:45:17.927599Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"We can see that indeed all those variables show a correlation coefficient higher than 0.8 with each other.","metadata":{}},{"cell_type":"code","source":"# in the features_to_drop_ the transformer stores all the\n# variables that will be dropped. \n\n# the transformer selects 1 variable per group of correlated ones\n# and drops the rest on a first come, first serve basis\nlen(correlated.features_to_drop_)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:45:26.051744Z","iopub.execute_input":"2022-02-15T19:45:26.052700Z","iopub.status.idle":"2022-02-15T19:45:26.061432Z","shell.execute_reply.started":"2022-02-15T19:45:26.052642Z","shell.execute_reply":"2022-02-15T19:45:26.060127Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# remove correlated variables\nprint('Number of variables before removing correlated: ', X_train.shape[1])\n\nX_train = correlated.transform(X_train)\nX_test = correlated.transform(X_test)\n\nprint('Number of variables after removing correlated: ', X_train.shape[1])","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:45:29.415314Z","iopub.execute_input":"2022-02-15T19:45:29.415971Z","iopub.status.idle":"2022-02-15T19:45:29.480863Z","shell.execute_reply.started":"2022-02-15T19:45:29.415913Z","shell.execute_reply":"2022-02-15T19:45:29.479669Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## Drop Correlated Features Smartly\n\nWith this class, each feature in the correlated group is selected based on different characteristics:\n\n- the number of missing values\n- the variance\n- the cardinality\n- the importance derived from a machine learning model\n\nThe transformer will select the feature with less missing values, or highest variance, cardinality or performance, depending what we choose on the selection_method parameter.\n","metadata":{}},{"cell_type":"code","source":"smart_corr = SmartCorrelatedSelection(\n    variables=None, # examines all variables\n    method=\"pearson\", # the correlation method\n    threshold=0.7, # the correlation coefficient threshold\n    missing_values=\"ignore\",\n    selection_method=\"model_performance\", # how to select the features\n    estimator=RandomForestClassifier(n_estimators=10, random_state=1), # the model from which to derive the importance\n)\n\n# find correlated features and select the best from each group\n\n# the method builds a random forest using each single feature from the correlated feature group\n# and retains the feature from the group with the best performance\nsmart_corr.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:46:45.863823Z","iopub.execute_input":"2022-02-15T19:46:45.864368Z","iopub.status.idle":"2022-02-15T19:46:57.246060Z","shell.execute_reply.started":"2022-02-15T19:46:45.864331Z","shell.execute_reply":"2022-02-15T19:46:57.245052Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# the correlated feature groups\nsmart_corr.correlated_feature_sets_","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:46:57.248067Z","iopub.execute_input":"2022-02-15T19:46:57.248386Z","iopub.status.idle":"2022-02-15T19:46:57.255713Z","shell.execute_reply.started":"2022-02-15T19:46:57.248353Z","shell.execute_reply":"2022-02-15T19:46:57.254753Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# lets examine the performace of a random forest based on\n# each feature from the fifth group from above, to understand\n# what the transformer is doing\n\n# select fifth group of correlated features\ngroup = smart_corr.correlated_feature_sets_[4]\n\n# build random forest with cross validation for\n# each feature\nfor f in group:    \n    model = cross_validate(\n        RandomForestClassifier(n_estimators=10, random_state=1),\n        X_train[f].to_frame(),\n        y_train,\n        cv=3,\n        return_estimator=False,\n        scoring='roc_auc',\n    )\n\n    print(f, model[\"test_score\"].mean())","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:47:26.734141Z","iopub.execute_input":"2022-02-15T19:47:26.734583Z","iopub.status.idle":"2022-02-15T19:47:27.506412Z","shell.execute_reply.started":"2022-02-15T19:47:26.734543Z","shell.execute_reply":"2022-02-15T19:47:27.504651Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"The variable **num_var30_0** returns the highest performing random forest, therefore this one will be retained and the other ones removed.","metadata":{}},{"cell_type":"code","source":"# this variable, which shows the best performance will be retained\n# and thus is not in the features_to_drop_ attribute\n'num_var30_0' in smart_corr.features_to_drop_","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:47:04.196059Z","iopub.execute_input":"2022-02-15T19:47:04.196612Z","iopub.status.idle":"2022-02-15T19:47:04.202819Z","shell.execute_reply.started":"2022-02-15T19:47:04.196572Z","shell.execute_reply":"2022-02-15T19:47:04.201770Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# this variable will be dropped, and thus it is in the features_to_drop_ attribute\n'ind_var12_0' in smart_corr.features_to_drop_","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:47:13.212676Z","iopub.execute_input":"2022-02-15T19:47:13.213090Z","iopub.status.idle":"2022-02-15T19:47:13.219632Z","shell.execute_reply.started":"2022-02-15T19:47:13.213050Z","shell.execute_reply":"2022-02-15T19:47:13.218756Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# this variable will be dropped, and thus it is in the features_to_drop_ attribute\n'ind_var24_0' in smart_corr.features_to_drop_","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:47:17.355985Z","iopub.execute_input":"2022-02-15T19:47:17.356754Z","iopub.status.idle":"2022-02-15T19:47:17.366107Z","shell.execute_reply.started":"2022-02-15T19:47:17.356680Z","shell.execute_reply":"2022-02-15T19:47:17.365052Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# remove correlated variables\nprint('Number of variables before removing correlated: ', X_train.shape[1])\n\nX_train = smart_corr.transform(X_train)\nX_test = smart_corr.transform(X_test)\n\nprint('Number of variables after removing correlated: ', X_train.shape[1])","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:47:43.978277Z","iopub.execute_input":"2022-02-15T19:47:43.978911Z","iopub.status.idle":"2022-02-15T19:47:44.012324Z","shell.execute_reply.started":"2022-02-15T19:47:43.978842Z","shell.execute_reply":"2022-02-15T19:47:44.010762Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"## Select features in a pipeline\n\nWe can perform all feature selection procedures in 1 step using a Pipeline from Scikit-learn.","metadata":{}},{"cell_type":"code","source":"# load data again\ndata = pd.read_csv('/kaggle/input/santander-customer-satisfaction/train.csv')\n\n# separate dataset into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['ID','TARGET'], axis=1),\n    data['TARGET'],\n    test_size=0.3,\n    random_state=0)\n\nX_train.shape, X_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:47:55.628701Z","iopub.execute_input":"2022-02-15T19:47:55.629092Z","iopub.status.idle":"2022-02-15T19:47:57.950337Z","shell.execute_reply.started":"2022-02-15T19:47:55.629058Z","shell.execute_reply":"2022-02-15T19:47:57.949416Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"pipe = Pipeline([\n    ('constant', DropConstantFeatures(tol=0.998)), # drops constand and quasi-constant altogether\n    ('duplicated', DropDuplicateFeatures()), # drops duplicates\n    ('correlation', SmartCorrelatedSelection( # drops correlated\n        threshold=0.8,\n        selection_method=\"model_performance\",\n        estimator=RandomForestClassifier(n_estimators=10, random_state=1),\n    )),\n])\n\n# find features to remove\npipe.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:48:15.195163Z","iopub.execute_input":"2022-02-15T19:48:15.195608Z","iopub.status.idle":"2022-02-15T19:48:59.982254Z","shell.execute_reply.started":"2022-02-15T19:48:15.195569Z","shell.execute_reply":"2022-02-15T19:48:59.981029Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# remove variables\nprint('Number of original variables: ', X_train.shape[1])\n\nX_train = pipe.transform(X_train)\nX_test = pipe.transform(X_test)\n\nprint('Number of variables after selection: ', X_train.shape[1])","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:48:59.984469Z","iopub.execute_input":"2022-02-15T19:48:59.984822Z","iopub.status.idle":"2022-02-15T19:49:00.193144Z","shell.execute_reply.started":"2022-02-15T19:48:59.984790Z","shell.execute_reply":"2022-02-15T19:49:00.192071Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"We can appreciate how in 1 cells we chopped down the number of features from 369 to 81.\n\n## Select features by Shuffling\n\nThis class, builds a model with all features, then shuffles each feature, one at a time, and determines a drop in model performance. If the feature is important, we should see a big drop. Otherwise, the drop will be small, and we could remove the feature.\n","metadata":{}},{"cell_type":"code","source":"# load data again\ndata = pd.read_csv('/kaggle/input/santander-customer-satisfaction/train.csv')\n\n# separate dataset into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['ID','TARGET'], axis=1),\n    data['TARGET'],\n    test_size=0.3,\n    random_state=0)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:49:00.194529Z","iopub.execute_input":"2022-02-15T19:49:00.194850Z","iopub.status.idle":"2022-02-15T19:49:02.478157Z","shell.execute_reply.started":"2022-02-15T19:49:00.194821Z","shell.execute_reply":"2022-02-15T19:49:02.477062Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# let's remove constant, quasi-constant and duplicates to speed things up\npipe = Pipeline([\n    ('constant', DropConstantFeatures(tol=0.998)), # drops constand and quasi-constant altogether\n    ('duplicated', DropDuplicateFeatures()),\n])\n\n# find features to remove\npipe.fit(X_train, y_train)\n\n# remove variables\nX_train = pipe.transform(X_train)\nX_test = pipe.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:49:08.081276Z","iopub.execute_input":"2022-02-15T19:49:08.081645Z","iopub.status.idle":"2022-02-15T19:49:10.683531Z","shell.execute_reply.started":"2022-02-15T19:49:08.081612Z","shell.execute_reply":"2022-02-15T19:49:10.682288Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"shuffle = SelectByShuffling(\n    estimator = RandomForestClassifier(n_estimators=10, max_depth=2, random_state=1), # the model\n    scoring=\"roc_auc\", # the metric to determine model performance\n    cv=3, # the cross-validation fold\n)\n\nshuffle.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:49:13.831392Z","iopub.execute_input":"2022-02-15T19:49:13.831800Z","iopub.status.idle":"2022-02-15T19:50:13.291145Z","shell.execute_reply.started":"2022-02-15T19:49:13.831764Z","shell.execute_reply":"2022-02-15T19:50:13.290186Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# this is the performace of the model (roc-auc) using all the features\nshuffle.initial_model_performance_","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:50:13.293302Z","iopub.execute_input":"2022-02-15T19:50:13.293600Z","iopub.status.idle":"2022-02-15T19:50:13.299612Z","shell.execute_reply.started":"2022-02-15T19:50:13.293571Z","shell.execute_reply":"2022-02-15T19:50:13.298802Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# in the attribute performance_drifts_ we can find the \n# performance drift caused by shuffling each feature\nshuffle.performance_drifts_","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:50:13.301692Z","iopub.execute_input":"2022-02-15T19:50:13.302057Z","iopub.status.idle":"2022-02-15T19:50:13.324107Z","shell.execute_reply.started":"2022-02-15T19:50:13.302023Z","shell.execute_reply":"2022-02-15T19:50:13.322942Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"pd.Series(shuffle.performance_drifts_).plot.bar(figsize=(20,5))\nplt.ylabel('Performance drift after shuffling')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:50:13.325362Z","iopub.execute_input":"2022-02-15T19:50:13.325849Z","iopub.status.idle":"2022-02-15T19:50:15.683921Z","shell.execute_reply.started":"2022-02-15T19:50:13.325802Z","shell.execute_reply":"2022-02-15T19:50:15.683079Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# here we find the attributes that will be dropped\nlen(shuffle.features_to_drop_)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:50:15.685759Z","iopub.execute_input":"2022-02-15T19:50:15.686102Z","iopub.status.idle":"2022-02-15T19:50:15.692465Z","shell.execute_reply.started":"2022-02-15T19:50:15.686070Z","shell.execute_reply":"2022-02-15T19:50:15.691029Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# remove variables\nprint('Number of variables before removing non important: ', X_train.shape[1])\n\nX_train = shuffle.transform(X_train)\nX_test = shuffle.transform(X_test)\n\nprint('Number of variables after removing non important: ', X_train.shape[1])","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:50:15.694202Z","iopub.execute_input":"2022-02-15T19:50:15.694542Z","iopub.status.idle":"2022-02-15T19:50:15.746130Z","shell.execute_reply.started":"2022-02-15T19:50:15.694510Z","shell.execute_reply":"2022-02-15T19:50:15.745204Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# we can go ahead and train a random forest using the selected features and evaluate\n# its performance\nrf = RandomForestClassifier(n_estimators=10, max_depth=3, random_state=1)\n\nrf.fit(X_train, y_train)\n\npred = rf.predict_proba(X_train)\nprint('Train roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n\npred = rf.predict_proba(X_test)\nprint('Test roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:50:15.747970Z","iopub.execute_input":"2022-02-15T19:50:15.748296Z","iopub.status.idle":"2022-02-15T19:50:15.995703Z","shell.execute_reply.started":"2022-02-15T19:50:15.748263Z","shell.execute_reply":"2022-02-15T19:50:15.994722Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"We see that the model with few features shows better performance than the model with all the features. And, it is much simpler and easier to interpret for those who will actually use the model.\n\n## Select features by univariate model performance\n\nThis selection procedure builds 1 model per feature, and selects those features that return models with a performance above a certain threshold.","metadata":{}},{"cell_type":"code","source":"# load data again\ndata = pd.read_csv('/kaggle/input/santander-customer-satisfaction/train.csv')\n\n# separate dataset into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['ID','TARGET'], axis=1),\n    data['TARGET'],\n    test_size=0.3,\n    random_state=0)\n\n# let's remove constant, quasi-constant and duplicates to speed things up\npipe = Pipeline([\n    ('constant', DropConstantFeatures(tol=0.998)), # drops constand and quasi-constant altogether\n    ('duplicated', DropDuplicateFeatures()),\n])\n\n# find features to remove\npipe.fit(X_train, y_train)\n\n# remove variables\nX_train = pipe.transform(X_train)\nX_test = pipe.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:50:15.997185Z","iopub.execute_input":"2022-02-15T19:50:15.997514Z","iopub.status.idle":"2022-02-15T19:50:20.795674Z","shell.execute_reply.started":"2022-02-15T19:50:15.997482Z","shell.execute_reply":"2022-02-15T19:50:20.794489Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"sel = SelectBySingleFeaturePerformance(\n    estimator = RandomForestClassifier(n_estimators=10, max_depth=2, random_state=1), # the model\n    scoring=\"roc_auc\", # the metric to determine model performance\n    cv=3, # the cross-validation fold,\n    threshold=None, # the performance threshold\n)\n\nsel.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:50:20.797232Z","iopub.execute_input":"2022-02-15T19:50:20.797682Z","iopub.status.idle":"2022-02-15T19:51:06.613342Z","shell.execute_reply.started":"2022-02-15T19:50:20.797636Z","shell.execute_reply":"2022-02-15T19:51:06.612318Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# the univariate performance of the features\nsel.feature_performance_","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:51:06.615734Z","iopub.execute_input":"2022-02-15T19:51:06.616088Z","iopub.status.idle":"2022-02-15T19:51:06.630826Z","shell.execute_reply.started":"2022-02-15T19:51:06.616055Z","shell.execute_reply":"2022-02-15T19:51:06.629381Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"pd.Series(sel.feature_performance_).plot.bar(figsize=(20,5))\nplt.ylabel('roc-auc')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:51:06.633055Z","iopub.execute_input":"2022-02-15T19:51:06.633665Z","iopub.status.idle":"2022-02-15T19:51:08.869486Z","shell.execute_reply.started":"2022-02-15T19:51:06.633620Z","shell.execute_reply":"2022-02-15T19:51:08.868159Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"# the features that will be dropped\nlen(sel.features_to_drop_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# when we leave the threshold to None, the selector selects features which\n# performance is bigger than the mean performance of all features\nsel.threshold","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove variables\nprint('Number of variables before removing non important: ', X_train.shape[1])\n\nX_train = sel.transform(X_train)\nX_test = sel.transform(X_test)\n\nprint('Number of variables after removing non important: ', X_train.shape[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Select Features Recursively\n\n- This method starts by building a model with all features\n- Then it ranks features by importance, derived from the model, from most to least important\n- Then removes least important features\n- Trains a new model and determines performance\n- If performance drop is big, then retains the feature, otherwise it removes it\n- Repeats steps 3-5 untill all features have been examined.","metadata":{}},{"cell_type":"code","source":"# load data again\ndata = pd.read_csv('/kaggle/input/santander-customer-satisfaction/train.csv')\n\n# separate dataset into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['ID','TARGET'], axis=1),\n    data['TARGET'],\n    test_size=0.3,\n    random_state=0)\n\n# let's remove constant, quasi-constant and duplicates to speed things up\npipe = Pipeline([\n    ('constant', DropConstantFeatures(tol=0.998)), # drops constand and quasi-constant altogether\n    ('duplicated', DropDuplicateFeatures()),\n])\n\n# find features to remove\npipe.fit(X_train, y_train)\n\n# remove variables\nX_train = pipe.transform(X_train)\nX_test = pipe.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfe = RecursiveFeatureElimination(\n    estimator = RandomForestClassifier(n_estimators=10, max_depth=2, random_state=1), # the model\n    scoring=\"roc_auc\", # the metric to determine model performance\n    cv=3, # the cross-validation fold\n    threshold = 0.04, \n)\n\nrfe.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the feature importance derived from the first model, trained\n# using all the features\nrfe.feature_importances_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot of feature importance, derived from the Random Forests\npd.Series(rfe.feature_importances_).plot.bar(figsize=(20,5))\nplt.ylabel('Feature importance derived from the random forests')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model begins by removing features, 1 by 1, from those on the left, to those on the right.","metadata":{}},{"cell_type":"code","source":"# the performance of the random forest trained on all features\nrfe.initial_model_performance_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the drop in performance caused when removing each feature\nrfe.performance_drifts_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# same as above in a plot\npd.Series(rfe.performance_drifts_).sort_values().plot.bar(figsize=(20,5))\nplt.ylabel('change in performance when removing feature')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the number of features that will be dropped\nlen(rfe.features_to_drop_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove variables\nprint('Number of variables before removing non important: ', X_train.shape[1])\n\nX_train = rfe.transform(X_train)\nX_test = rfe.transform(X_test)\n\nprint('Number of variables after removing non important: ', X_train.shape[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Selection and Machine Learning Pipeline\n\nNow we will select features and train a machine learning model altogether in 1 pipeline.","metadata":{}},{"cell_type":"code","source":"# load data again\ndata = pd.read_csv('/kaggle/input/santander-customer-satisfaction/train.csv')\n\n# separate dataset into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['ID','TARGET'], axis=1),\n    data['TARGET'],\n    test_size=0.3,\n    random_state=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe = Pipeline([\n    # ======== FEATURE SELECTION =======\n    ('constant', DropConstantFeatures(tol=0.998)), # drops constand and quasi-constant altogether\n    ('duplicated', DropDuplicateFeatures()), # drop duplicated\n    ('shuffle', SelectByShuffling( # select by feature shuffling\n        estimator = RandomForestClassifier(n_estimators=10, max_depth=2, random_state=1), # the model\n        scoring=\"roc_auc\", # the metric to determine model performance\n        cv=3, # the cross-validation fold\n    )),\n    \n    # =====  the machine learning model ====\n    ('random_forest', RandomForestClassifier(n_estimators=10, max_depth=2, random_state=1)),\n])\n\n# find features to remove\npipe.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the pipeline takes in the raw data, removes all unwanted features and then\n# makes the prediction with the model trained on the final subset of variables\n\n# obtain predictions and determine model performance\npred = pipe.predict_proba(X_train)\nprint('Train roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n\npred = pipe.predict_proba(X_test)\nprint('Test roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That is all for now. I hope you find this notebook and this library useful. If you do, please upvote the notebook :)\n\n\n## References and further reading\n\n\n- [Feature-engine](https://feature-engine.readthedocs.io/en/latest/index.html), Python open-source library\n- [Feature Selection for Machine Learning](https://www.udemy.com/course/feature-selection-for-machine-learning/?referralCode=186501DF5D93F48C4F71), Online Course\n- [Comprehensive Guide on Feature Selection](https://www.kaggle.com/prashant111/comprehensive-guide-on-feature-selection), Kaggle notebook","metadata":{}}]}