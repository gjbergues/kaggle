{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# https://www.kaggle.com/joparga3/in-depth-skewed-data-classif-93-recall-acc-now/notebook\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-18T22:52:16.412019Z","iopub.execute_input":"2022-01-18T22:52:16.412541Z","iopub.status.idle":"2022-01-18T22:52:16.424344Z","shell.execute_reply.started":"2022-01-18T22:52:16.412507Z","shell.execute_reply":"2022-01-18T22:52:16.423545Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"**Why do I use this kernel?**\n\nI have a deeply interest in find out if a transaction is a fraud or not. \nAbout the Data set: \n*     it contains transactions made by credit cards in September 2013 by european cardholders.\n*     It's highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n*     It contains only numerical input variables which are the result of a PCA transformation (confidential data).","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-01-18T22:52:16.426006Z","iopub.execute_input":"2022-01-18T22:52:16.426286Z","iopub.status.idle":"2022-01-18T22:52:16.433185Z","shell.execute_reply.started":"2022-01-18T22:52:16.426256Z","shell.execute_reply":"2022-01-18T22:52:16.432204Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Read data\ndf = pd.read_csv('/kaggle/input/creditcardfraud/creditcard.csv')","metadata":{"execution":{"iopub.status.busy":"2022-01-18T22:52:16.434691Z","iopub.execute_input":"2022-01-18T22:52:16.435087Z","iopub.status.idle":"2022-01-18T22:52:18.897208Z","shell.execute_reply.started":"2022-01-18T22:52:16.435030Z","shell.execute_reply":"2022-01-18T22:52:18.896278Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# Check the columns and general structure\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T22:52:18.900866Z","iopub.execute_input":"2022-01-18T22:52:18.901121Z","iopub.status.idle":"2022-01-18T22:52:18.928097Z","shell.execute_reply.started":"2022-01-18T22:52:18.901091Z","shell.execute_reply":"2022-01-18T22:52:18.927269Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"### As we can see, the data is confidential, we don't know the column names.","metadata":{}},{"cell_type":"code","source":"# Search missing values\ndf.isnull().sum().max()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T22:52:18.930950Z","iopub.execute_input":"2022-01-18T22:52:18.931278Z","iopub.status.idle":"2022-01-18T22:52:18.955813Z","shell.execute_reply.started":"2022-01-18T22:52:18.931233Z","shell.execute_reply":"2022-01-18T22:52:18.954858Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"### No missing values","metadata":{}},{"cell_type":"markdown","source":"# See if the data is balanced or unbalanced","metadata":{}},{"cell_type":"code","source":"from matplotlib.legend_handler import HandlerBase\nfrom matplotlib.text import Text\n\n# Class for create special legend on the right of the fig.\nclass TextHandler(HandlerBase):\n    def create_artists(self, legend, tup ,xdescent, ydescent, width, height, fontsize,trans):\n        tx = Text(width/2.,height/2,tup[0], fontsize=fontsize,\n                  ha=\"center\", va=\"center\", color=tup[1], fontweight=\"bold\")\n        return [tx]\n\n# Countplot\nax = sns.countplot(x=\"Class\", data=df)\nplt.title(\"Frauds vs Not. Frauds\")\nplt.xlabel(\"Target\")\nplt.ylabel(\"Count\")\nsns.set(rc = {'figure.figsize':(8,8)})\n#Add quantity to each bar\nfor p in ax.patches:\n   ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.25, p.get_height()+0.01))\n\n# Add right legend\nhandltext = [\"0\", \"1\"]\nlabels = [\"Not Fraud\", \"Fraud\"]\n\nt = ax.get_xticklabels()\nlabeldic = dict(zip(handltext, labels))\nlabels = [labeldic[h.get_text()]  for h in t]\nhandles = [(h.get_text(),c.get_fc()) for h,c in zip(t,ax.patches)]\n\nax.legend(handles, labels, handler_map={tuple : TextHandler()}) \n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-18T22:52:18.957376Z","iopub.execute_input":"2022-01-18T22:52:18.957693Z","iopub.status.idle":"2022-01-18T22:52:19.213243Z","shell.execute_reply.started":"2022-01-18T22:52:18.957651Z","shell.execute_reply":"2022-01-18T22:52:19.212421Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# The classes are heavily skewed we need to solve this issue.\nprint('No Frauds', round(df['Class'].value_counts()[0]/len(df) * 100,2), '% of the dataset')\nprint('Frauds', round(df['Class'].value_counts()[1]/len(df) * 100,2), '% of the dataset')","metadata":{"execution":{"iopub.status.busy":"2022-01-18T22:52:19.214345Z","iopub.execute_input":"2022-01-18T22:52:19.214555Z","iopub.status.idle":"2022-01-18T22:52:19.226857Z","shell.execute_reply.started":"2022-01-18T22:52:19.214528Z","shell.execute_reply":"2022-01-18T22:52:19.226301Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"**Distributions:** By seeing the distributions we can have an idea how skewed are these features, we can also see further distributions of the other features. ","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(18,4))\n\namount_val = df['Amount'].values\ntime_val = df['Time'].values\n\nsns.distplot(amount_val, ax=ax[0], color='r')\nax[0].set_title('Distribution of Transaction Amount', fontsize=14)\nax[0].set_xlim([min(amount_val), max(amount_val)])\n\nsns.distplot(time_val, ax=ax[1], color='b')\nax[1].set_title('Distribution of Transaction Time', fontsize=14)\nax[1].set_xlim([min(time_val), max(time_val)])","metadata":{"execution":{"iopub.status.busy":"2022-01-18T22:52:19.227739Z","iopub.execute_input":"2022-01-18T22:52:19.228508Z","iopub.status.idle":"2022-01-18T22:52:22.227340Z","shell.execute_reply.started":"2022-01-18T22:52:19.228472Z","shell.execute_reply":"2022-01-18T22:52:22.226289Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Unbalance Data\n\n## What can we do?\n\n* Collect more data.\n* Use a correct metric (don't use accuracy):\n    * Use the confusio nmatrix to calculate Precision, Recall\n    * F1score (weighted average of precision recall)\n    * Use Kappa - which is a classification accuracy normalized by the imbalance of the classes in the data\n    * ROC curves - calculates sensitivity/specificity ratio.\n* Resampling the dataset (process the data to have an approximate 50-50 ratio).\n    * OVER-sampling, adding copies of the under-represented class (better when you have little data).\n    * UNDER-sampling, deletes instances from the over-represented class (better when he have lot's of data).\n","metadata":{}},{"cell_type":"markdown","source":"## Scaling and Distributing\nIt's necesary to scale columns Time and Amount. The other columns are already scaled (It was applied PCA). ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, RobustScaler\n\n# IMPORTANT: RobustScaler is less prone to outliers.\n\nstd_scaler = StandardScaler()\nrob_scaler = RobustScaler()\n\n# reshape to avoid ValueError: Expected 2D array, got 1D array instead:\ndf['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n\n# Erase old not scaled columns Time and Amount\ndf.drop(['Time','Amount'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T22:52:22.228863Z","iopub.execute_input":"2022-01-18T22:52:22.229434Z","iopub.status.idle":"2022-01-18T22:52:22.408937Z","shell.execute_reply.started":"2022-01-18T22:52:22.229383Z","shell.execute_reply":"2022-01-18T22:52:22.407933Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T22:52:22.410189Z","iopub.execute_input":"2022-01-18T22:52:22.410418Z","iopub.status.idle":"2022-01-18T22:52:22.441127Z","shell.execute_reply.started":"2022-01-18T22:52:22.410390Z","shell.execute_reply":"2022-01-18T22:52:22.440281Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"scaled_amount = df['scaled_amount']\nscaled_time = df['scaled_time']\n\ndf.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\ndf.insert(0, 'scaled_amount', scaled_amount)\ndf.insert(1, 'scaled_time', scaled_time)\n\n# Amount and Time are Scaled!\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T22:52:22.443066Z","iopub.execute_input":"2022-01-18T22:52:22.443954Z","iopub.status.idle":"2022-01-18T22:52:22.512937Z","shell.execute_reply.started":"2022-01-18T22:52:22.443902Z","shell.execute_reply":"2022-01-18T22:52:22.512190Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"## Sub-Sample the data\n\nIt will be a dataframe with a 50/50 ratio of fraud and non-fraud transactions. \n\n## Why do we create a sub-Sample?\n\nUsing the original dataframe will cause the following issues:\n\n* **Overfitting:** Our classification models will assume that in most cases there are no frauds! What we want for our model is to be certain when a fraud occurs.\n* **Wrong Correlations:** Although we don't know what the \"V\" features stand for, it will be useful to understand how each of this features influence the result (Fraud or No Fraud) by having an imbalance dataframe we are not able to see the true correlations between the class and features.","metadata":{}},{"cell_type":"markdown","source":"# Splitting the Data (Original DataFrame)\n\nBefore proceeding with the Random UnderSampling technique we have to separate the orginal dataframe. Why? for testing purposes, remember although we are splitting the data when implementing Random UnderSampling or OverSampling techniques, we want to test our models on the original testing set not on the testing set created by either of these techniques. The main goal is to fit the model either with the dataframes that were undersample and oversample (in order for our models to detect the patterns), and test it on the original testing set.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\nprint('No Frauds', round(df['Class'].value_counts()[0]/len(df) * 100,2), '% of the dataset')\nprint('Frauds', round(df['Class'].value_counts()[1]/len(df) * 100,2), '% of the dataset')\n\nX = df.drop('Class', axis=1)\ny = df['Class']\n\nsss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n\nfor train_index, test_index in sss.split(X, y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n\n# We already have X_train and y_train for undersample data thats why I am using original to distinguish and to not overwrite these variables.\n# original_Xtrain, original_Xtest, original_ytrain, original_ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Check the Distribution of the labels\n\n\n# Turn into an array\noriginal_Xtrain = original_Xtrain.values\noriginal_Xtest = original_Xtest.values\noriginal_ytrain = original_ytrain.values\noriginal_ytest = original_ytest.values\n\n# See if both the train and test label distribution are similarly distributed\ntrain_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\ntest_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\nprint('-' * 100)\n\nprint('Label Distributions: \\n')\nprint(train_counts_label/ len(original_ytrain))\nprint(test_counts_label/ len(original_ytest))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T22:52:22.514034Z","iopub.execute_input":"2022-01-18T22:52:22.514553Z","iopub.status.idle":"2022-01-18T22:52:22.894588Z","shell.execute_reply.started":"2022-01-18T22:52:22.514518Z","shell.execute_reply":"2022-01-18T22:52:22.893614Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"# Random Under-Sampling:\n\n\nIn this phase of the project we will implement \"Random Under Sampling\" which basically consists of removing data in order to have a more balanced dataset and thus avoiding our models to overfitting.\n\nSteps:\n* The first thing we have to do is determine how imbalanced is our class (use \"value_counts()\" on the class column to determine the amount for each label)\n* Once we determine how many instances are considered fraud transactions (Fraud = \"1\") , we should bring the non-fraud transactions to the same amount as fraud transactions (assuming we want a 50/50 ratio), this will be equivalent to 492 cases of fraud and 492 cases of non-fraud transactions.\n* After implementing this technique, we have a sub-sample of our dataframe with a 50/50 ratio with regards to our classes. Then the next step we will implement is to shuffle the data to see if our models can maintain a certain accuracy everytime we run this script.\n\n### Note: \n\n**The main issue with \"Random Under-Sampling\" is that we run the risk that our classification models will not perform as accurate as we would like to since there is a great deal of information loss (bringing 492 non-fraud transaction from 284,315 non-fraud transaction)**\n","metadata":{}},{"cell_type":"code","source":"# Since our classes are highly skewed we should make them equivalent in order to have a normal distribution of the classes.\n\n# Lets shuffle the data before creating the subsamples\n\ndf = df.sample(frac=1)\n\n# amount of fraud classes 492 rows.\nfraud_df = df.loc[df['Class'] == 1]\nnon_fraud_df = df.loc[df['Class'] == 0][:492]\n\nnormal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n\n# Shuffle dataframe rows\nnew_df = normal_distributed_df.sample(frac=1, random_state=42)\n\nnew_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T22:52:22.896234Z","iopub.execute_input":"2022-01-18T22:52:22.896565Z","iopub.status.idle":"2022-01-18T22:52:23.051523Z","shell.execute_reply.started":"2022-01-18T22:52:22.896520Z","shell.execute_reply":"2022-01-18T22:52:23.050481Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"ax = sns.countplot(x=\"Class\", data=new_df)\nplt.title(\"Frauds vs Not. Frauds\")\nplt.xlabel(\"Target\")\nplt.ylabel(\"Count\")\nsns.set(rc = {'figure.figsize':(8,8)})\n#Add value counts to each bar\nfor p in ax.patches:\n   ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.25, p.get_height()+0.01))\n\n# Add right legend\nhandltext = [\"0\", \"1\"]\nlabels = [\"Not Fraud\", \"Fraud\"]\n\nt = ax.get_xticklabels()\nlabeldic = dict(zip(handltext, labels))\nlabels = [labeldic[h.get_text()]  for h in t]\nhandles = [(h.get_text(),c.get_fc()) for h,c in zip(t,ax.patches)]\n\nax.legend(handles, labels, handler_map={tuple : TextHandler()}) \n","metadata":{"execution":{"iopub.status.busy":"2022-01-18T22:52:23.055929Z","iopub.execute_input":"2022-01-18T22:52:23.056183Z","iopub.status.idle":"2022-01-18T22:52:23.308987Z","shell.execute_reply.started":"2022-01-18T22:52:23.056155Z","shell.execute_reply":"2022-01-18T22:52:23.308045Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"## Correlation Matrices\n\nCorrelation matrices are the essence of understanding our data. We want to know if there are features that influence heavily in whether a specific transaction is a fraud. However, it is important that we use the correct dataframe (subsample) in order for us to see which features have a high positive or negative correlation with regards to fraud transactions.\n\nSummary and Explanation:\n\n* Negative Correlations: V17, V14, V12 and V10 are negatively correlated. Notice how the lower these values are, the more likely the end result will be a fraud transaction.\n\n* Positive Correlations: V2, V4, V11, and V19 are positively correlated. Notice how the higher these values are, the more likely the end result will be a fraud transaction.\n\n* BoxPlots: We will use boxplots to have a better understanding of the distribution of these features in fradulent and non fradulent transactions.\n\n### Note: \n**We have to make sure we use the subsample in our correlation matrix or else our correlation matrix will be affected by the high imbalance between our classes. This occurs due to the high class imbalance in the original dataframe.**","metadata":{}},{"cell_type":"code","source":"# Make sure we use the subsample in our correlation\n\nf, (ax1, ax2) = plt.subplots(2, 1, figsize=(24,20))\n\n# Entire DataFrame\ncorr = df.corr()\nsns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax1)\nax1.set_title(\"Imbalanced Correlation Matrix \\n (don't use for reference)\", fontsize=14)\n\n\nsub_sample_corr = new_df.corr()\nsns.heatmap(sub_sample_corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax2)\nax2.set_title('SubSample Correlation Matrix \\n (use for reference)', fontsize=14)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T22:56:40.410480Z","iopub.execute_input":"2022-01-18T22:56:40.410768Z","iopub.status.idle":"2022-01-18T22:56:42.905543Z","shell.execute_reply.started":"2022-01-18T22:56:40.410738Z","shell.execute_reply":"2022-01-18T22:56:42.904981Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"f, axes = plt.subplots(ncols=4, figsize=(20,4))\n\n# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)\nsns.boxplot(x=\"Class\", y=\"V17\", data=new_df, ax=axes[0])\naxes[0].set_title('V17 vs Class Negative Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V14\", data=new_df, ax=axes[1])\naxes[1].set_title('V14 vs Class Negative Correlation')\n\n\nsns.boxplot(x=\"Class\", y=\"V12\", data=new_df,  ax=axes[2])\naxes[2].set_title('V12 vs Class Negative Correlation')\n\n\nsns.boxplot(x=\"Class\", y=\"V10\", data=new_df, ax=axes[3])\naxes[3].set_title('V10 vs Class Negative Correlation')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T23:00:20.488170Z","iopub.execute_input":"2022-01-18T23:00:20.488843Z","iopub.status.idle":"2022-01-18T23:00:21.224720Z","shell.execute_reply.started":"2022-01-18T23:00:20.488806Z","shell.execute_reply":"2022-01-18T23:00:21.223941Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}