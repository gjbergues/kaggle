{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Study Kernel**\n\n**Why do I study this kernel?**\n\nI have a deeply interest in find out if a transaction is a fraud or not. \nAbout the Data set: \n*     it contains transactions made by credit cards in September 2013 by european cardholders.\n*     It's highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n*     It contains only numerical input variables which are the result of a PCA transformation (confidential data).\n\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-02-21T15:35:55.086037Z","iopub.execute_input":"2022-02-21T15:35:55.086670Z","iopub.status.idle":"2022-02-21T15:35:56.252819Z","shell.execute_reply.started":"2022-02-21T15:35:55.086540Z","shell.execute_reply":"2022-02-21T15:35:56.251886Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Read data\ndf = pd.read_csv('/kaggle/input/creditcardfraud/creditcard.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-21T15:36:01.755515Z","iopub.execute_input":"2022-02-21T15:36:01.755961Z","iopub.status.idle":"2022-02-21T15:36:06.142230Z","shell.execute_reply.started":"2022-02-21T15:36:01.755918Z","shell.execute_reply":"2022-02-21T15:36:06.141525Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Check the columns and general structure\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T15:36:11.266964Z","iopub.execute_input":"2022-02-21T15:36:11.267582Z","iopub.status.idle":"2022-02-21T15:36:11.311153Z","shell.execute_reply.started":"2022-02-21T15:36:11.267508Z","shell.execute_reply":"2022-02-21T15:36:11.310542Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### As we can see, we don't know the meaning of the columns. The data is confidential.","metadata":{}},{"cell_type":"code","source":"# Search missing values\nprint(\"There are {} missing values\".format(df.isnull().sum().max()))","metadata":{"execution":{"iopub.status.busy":"2022-02-21T15:37:47.558791Z","iopub.execute_input":"2022-02-21T15:37:47.559161Z","iopub.status.idle":"2022-02-21T15:37:47.594600Z","shell.execute_reply.started":"2022-02-21T15:37:47.559128Z","shell.execute_reply":"2022-02-21T15:37:47.593673Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### **No missing values**","metadata":{}},{"cell_type":"markdown","source":"## **Check if the data is balanced or unbalanced**","metadata":{}},{"cell_type":"code","source":"# Plot a bar graph\nfrom matplotlib.legend_handler import HandlerBase\nfrom matplotlib.text import Text\n\n# Class for create special legend on the right of the Countplot fig.\nclass TextHandler(HandlerBase):\n    def create_artists(self, legend, tup ,xdescent, ydescent, width, height, fontsize,trans):\n        tx = Text(width/2., height/2, tup[0], fontsize=fontsize, ha=\"center\", va=\"center\", color=tup[1], fontweight=\"bold\")\n        return [tx]\n\n# Countplot\nax = sns.countplot(x=\"Class\", data=df)\nplt.title(\"Frauds vs Not. Frauds\")\nplt.xlabel(\"Target\")\nplt.ylabel(\"Count\")\nsns.set(rc = {'figure.figsize':(8,8)})\n#Add quantity to each bar\nfor p in ax.patches:\n   ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.25, p.get_height()+0.01))\n\n# Add right legend\nhandltext = [\"0\", \"1\"]\nlabels = [\"Not Fraud\", \"Fraud\"]\n\nt = ax.get_xticklabels()\nlabeldic = dict(zip(handltext, labels))\nlabels = [labeldic[h.get_text()]  for h in t]\nhandles = [(h.get_text(),c.get_fc()) for h,c in zip(t,ax.patches)]\n\nax.legend(handles, labels, handler_map={tuple : TextHandler()}) \n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-21T15:38:40.226655Z","iopub.execute_input":"2022-02-21T15:38:40.227006Z","iopub.status.idle":"2022-02-21T15:38:40.547150Z","shell.execute_reply.started":"2022-02-21T15:38:40.226966Z","shell.execute_reply":"2022-02-21T15:38:40.546184Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# The classes are heavily skewed.\n# Skewed = make biased or distorted in a way that is regarded as inaccurate, unfair, or misleading.\n# Percentage of Frauds and Non-Frauds\nprint('No Frauds', round(df['Class'].value_counts()[0]/len(df) * 100,2), '% of the dataset')\nprint('Frauds', round(df['Class'].value_counts()[1]/len(df) * 100,2), '% of the dataset')","metadata":{"execution":{"iopub.status.busy":"2022-02-21T15:39:29.933901Z","iopub.execute_input":"2022-02-21T15:39:29.934492Z","iopub.status.idle":"2022-02-21T15:39:29.947045Z","shell.execute_reply.started":"2022-02-21T15:39:29.934454Z","shell.execute_reply":"2022-02-21T15:39:29.946158Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## **Distributions** \n\nBy seeing the distributions we can have an id of how skewed are these features. ","metadata":{}},{"cell_type":"code","source":"# Time and Amount distribution\nfig, ax = plt.subplots(1, 2, figsize=(18,4))\n\namount_val = df['Amount'].values\ntime_val = df['Time'].values\n\nsns.distplot(amount_val, ax=ax[0], color='r')\nax[0].set_title('Distribution of Transaction Amount', fontsize=14)\nax[0].set_xlim([min(amount_val), max(amount_val)])\n\nsns.distplot(time_val, ax=ax[1], color='b')\nax[1].set_title('Distribution of Transaction Time', fontsize=14)\nax[1].set_xlim([min(time_val), max(time_val)])","metadata":{"execution":{"iopub.status.busy":"2022-02-21T15:40:26.485745Z","iopub.execute_input":"2022-02-21T15:40:26.486052Z","iopub.status.idle":"2022-02-21T15:40:29.659517Z","shell.execute_reply.started":"2022-02-21T15:40:26.486022Z","shell.execute_reply":"2022-02-21T15:40:29.658717Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# **Unbalance Data**\n\n## What can we do? Main ideas:\n\n* Collect more data (not possible for this data, we just have this quantity).\n* Use a correct metric (don't use accuracy):\n    * Accuracy = (TP + TN) / (TP + FN + FP + TN) --> \"It is not suitable because the impact of the least represented, but more important samples. The majority class is not the main sample.\"\n    * Use the confusion matrix to calculate Precision, Recall.\n    * **F1score** (weighted average of precision recall).\n    * Use **Kappa** - which is a classification accuracy normalized by the imbalance of the classes in the data.\n    * **ROC curves** - calculates sensitivity/specificity ratio.\n* Resampling the dataset (process the data to have an approximate 50-50 ratio).\n    * OVER-sampling, adding copies of the under-represented class (better when you have little data).\n    * UNDER-sampling, deletes instances from the over-represented class (better when he have lot's of data).\n","metadata":{}},{"cell_type":"markdown","source":"## Scaling and Distributing\n\n\"*If there are input variables that have very large values relative to the other input variables,<br> these large values can dominate or skew some machine learning algorithms.*\" <br> The result is that the algorithms pay most of their attention to the large values and ignore the variables with smaller values.\"\n\nIt's necesary to scale columns Time and Amount. The other columns are already scaled (data with PCA). ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\n\n# IMPORTANT: RobustScaler works better when the data have many outliers.\n\nrob_scaler = RobustScaler()\n\n# Reshape(-1, 1) to avoid ValueError: Expected 2D array, got 1D array instead:\ndf['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n\n# Erase old not scaled columns Time and Amount\ndf.drop(['Time','Amount'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T15:42:30.491521Z","iopub.execute_input":"2022-02-21T15:42:30.492683Z","iopub.status.idle":"2022-02-21T15:42:30.783795Z","shell.execute_reply.started":"2022-02-21T15:42:30.492619Z","shell.execute_reply":"2022-02-21T15:42:30.782825Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"scaled_amount = df['scaled_amount']\nscaled_time = df['scaled_time']\n\ndf.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\ndf.insert(0, 'scaled_amount', scaled_amount)\ndf.insert(1, 'scaled_time', scaled_time)\n\n# As we can see, amount and timer are scaled correctly\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T15:42:33.199777Z","iopub.execute_input":"2022-02-21T15:42:33.200126Z","iopub.status.idle":"2022-02-21T15:42:33.271716Z","shell.execute_reply.started":"2022-02-21T15:42:33.200091Z","shell.execute_reply":"2022-02-21T15:42:33.270712Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## **Sub-Sample the data**\n\nIt will be a dataframe with a 50/50 ratio of fraud and non-fraud transactions. \n\n## **Why do we create a sub-Sample?**\n\nUsing the original dataframe will cause the following issues:\n\n* **Overfitting:** Our classification models will assume that in most cases there are no frauds! What we want for our model is to be certain when a fraud occurs.\n* **Wrong Correlations:** Although we don't know what the \"V\" features stand for, it will be useful to understand how each one influences the result (Fraud or Non-Fraud). By having an imbalance dataframe we are not able to see the true correlations between the class and features.","metadata":{}},{"cell_type":"markdown","source":"# Splitting the Data (Original DataFrame)\n\nBefore proceeding with the **Random UnderSampling** technique we have to separate the orginal dataframe. <br>\n\nWhy? For testing purposes, remember although we are splitting the data when implementing Random UnderSampling or OverSampling techniques, we want to test our models on the original testing set not on the testing set created by either of these techniques. \n\nThe main goal is to fit the model either with the dataframes that were undersample and oversample (in order for our models to detect the patterns), and test it on the original testing set.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\nprint('No Frauds', round(df['Class'].value_counts()[0]/len(df) * 100,2), '% of the dataset')\nprint('Frauds', round(df['Class'].value_counts()[1]/len(df) * 100,2), '% of the dataset')\n\n# Split\nX = df.drop('Class', axis=1)\ny = df['Class']\n\n# StratifiedKFold Instance\nskf = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n\nfor train_index, test_index in skf.split(X, y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n\n# Turn into an array\noriginal_Xtrain = original_Xtrain.values\n\noriginal_Xtest = original_Xtest.values\noriginal_ytrain = original_ytrain.values\noriginal_ytest = original_ytest.values\n\n# See if both the train and test label distribution are similarly distributed\ntrain_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\ntest_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\nprint('-' * 100)\n# Check the Distribution of the labels\nprint('Label Distributions: \\n')\nprint(train_counts_label/ len(original_ytrain))\nprint(test_counts_label/ len(original_ytest))","metadata":{"execution":{"iopub.status.busy":"2022-02-21T15:43:44.973826Z","iopub.execute_input":"2022-02-21T15:43:44.974159Z","iopub.status.idle":"2022-02-21T15:43:45.387010Z","shell.execute_reply.started":"2022-02-21T15:43:44.974124Z","shell.execute_reply":"2022-02-21T15:43:45.386079Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# **Random Under-Sampling.**\n\n\nIt consists of removing data in order to have a more balanced dataset and thus avoiding our models to overfitting.\n\n**Steps:**\n1. Determine how imbalanced is our class (use \"value_counts()\" on the class column to determine the amount for each label)\n2. Bring the non-fraud transactions to the same amount as fraud transactions (assuming we want a 50/50 ratio).\n3. Shuffle the data to see if our models can maintain a certain accuracy everytime we run this script.\n\n### Note: \n**The main issue with \"Random Under-Sampling\" is that we run the risk that our classification models will not perform as accurate as we would like to since there is a great deal of information loss (bringing 492 non-fraud transaction from 284,315 non-fraud transaction)**\n","metadata":{}},{"cell_type":"code","source":"# Lets shuffle the data before creating the subsamples\ndf = df.sample(frac=1)\n\n# amount of fraud classes 492 rows.\nfraud_df = df.loc[df['Class'] == 1]\nnon_fraud_df = df.loc[df['Class'] == 0][:492]\n\nnormal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n\n# Shuffle dataframe rows\nnew_df = normal_distributed_df.sample(frac=1, random_state=42)\n\n# You can see the suffle\nnew_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T15:44:39.258804Z","iopub.execute_input":"2022-02-21T15:44:39.259148Z","iopub.status.idle":"2022-02-21T15:44:39.449853Z","shell.execute_reply.started":"2022-02-21T15:44:39.259115Z","shell.execute_reply":"2022-02-21T15:44:39.448764Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Checking the new distribution of the data\nax = sns.countplot(x=\"Class\", data=new_df)\nplt.title(\"Frauds vs Not. Frauds\")\nplt.xlabel(\"Target\")\nplt.ylabel(\"Count\")\nsns.set(rc = {'figure.figsize':(8,8)})\n#Add value counts to each bar\nfor p in ax.patches:\n   ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.25, p.get_height()+0.01))\n\n# Add right legend\nhandltext = [\"0\", \"1\"]\nlabels = [\"Not Fraud\", \"Fraud\"]\n\nt = ax.get_xticklabels()\nlabeldic = dict(zip(handltext, labels))\nlabels = [labeldic[h.get_text()]  for h in t]\nhandles = [(h.get_text(),c.get_fc()) for h,c in zip(t,ax.patches)]\n\nax.legend(handles, labels, handler_map={tuple : TextHandler()}) \n","metadata":{"execution":{"iopub.status.busy":"2022-02-21T15:44:52.885190Z","iopub.execute_input":"2022-02-21T15:44:52.885541Z","iopub.status.idle":"2022-02-21T15:44:53.147738Z","shell.execute_reply.started":"2022-02-21T15:44:52.885506Z","shell.execute_reply":"2022-02-21T15:44:53.146632Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## **Correlation Matrices**\n\nThey're important to understand the data. We want to know if there are features that influence heavily in whether a specific transaction is a fraud.<br> However, it is important that we use the correct dataframe (subsample) in order for us to see which features have a high positive or negative correlation with regards to fraud transactions.\n\nSummary and Explanation:\n\n* **Negative Correlations:** V17, V14, V12 and V10 are negatively correlated with the class(target). Notice how the lower these values are, the more likely the end result will be a fraud transaction.\n\n* **Positive Correlations:** V2, V4, V11, and V19 are positively correlated. Notice how the higher these values are, the more likely the end result will be a fraud transaction.\n\n* **BoxPlots:** We will use boxplots to have a better understanding of the distribution of these features in fradulent and non fradulent transactions.\n\n### Note: \n**We have to make sure we use the subsample in our correlation matrix or else our correlation matrix will be affected by the high imbalance between our classes. This occurs due to the high class imbalance in the original dataframe.**","metadata":{}},{"cell_type":"code","source":"# Make sure we use the subsample in our correlation\nf, (ax1, ax2) = plt.subplots(2, 1, figsize=(24,20))\n\n# Entire DataFrame\ncorr = df.corr()\nsns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax1)\nax1.set_title(\"Imbalanced Correlation Matrix \\n (don't use for reference)\", fontsize=14)\n\n\nsub_sample_corr = new_df.corr()\nsns.heatmap(sub_sample_corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax2)\nax2.set_title('SubSample Correlation Matrix \\n (use for reference)', fontsize=14)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T15:45:09.895056Z","iopub.execute_input":"2022-02-21T15:45:09.895442Z","iopub.status.idle":"2022-02-21T15:45:12.637380Z","shell.execute_reply.started":"2022-02-21T15:45:09.895406Z","shell.execute_reply":"2022-02-21T15:45:12.636541Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## **Box Plots**\n\nUsing the previous observations, we created a serie of box plots.","metadata":{}},{"cell_type":"code","source":"f, axes = plt.subplots(ncols=4, figsize=(20,4))\n\n# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)\nsns.boxplot(x=\"Class\", y=\"V17\", data=new_df, ax=axes[0])\naxes[0].set_title('V17 vs Class (Negative Correlation)')\n\nsns.boxplot(x=\"Class\", y=\"V14\", data=new_df, ax=axes[1])\naxes[1].set_title('V14 vs Class (Negative Correlation)')\n\nsns.boxplot(x=\"Class\", y=\"V12\", data=new_df,  ax=axes[2])\naxes[2].set_title('V12 vs Class (Negative Correlation)')\n\nsns.boxplot(x=\"Class\", y=\"V10\", data=new_df, ax=axes[3])\naxes[3].set_title('V10 vs Class (Negative Correlation)')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## We don't have outliers in V17, but in V10, V12 and v14 they are some of them.","metadata":{}},{"cell_type":"markdown","source":"## Anomaly Detection:\n\nOur main aim in this section is to remove \"extreme outliers\" from features that have a high correlation with our classes. This will have a positive impact on the accuracy of our models.  \n\n### Interquartile Range Method:\n<ul>\n<li> <b>Interquartile Range (IQR):</b> We calculate this by the difference between the 75th percentile and 25th percentile. Our aim is to create a threshold beyond the 75th and 25th percentile that in case some instance pass this threshold the instance will be deleted. </li>\n<li> <b>Boxplots:</b> Besides easily seeing the 25th and 75th percentiles (both end of the squares) it is also easy to see extreme outliers (points beyond the lower and higher extreme). </li>\n</ul>\n\n### Outlier Removal Tradeoff:\nWe have to be careful as to how far do we want the threshold for removing outliers. We determine the threshold by multiplying a number (ex: 1.5) by the (Interquartile Range). The higher this threshold is, the less outliers will detect (multiplying by a higher number ex: 3), and the lower this threshold is the more outliers it will detect.  <br>\n\n**The Tradeoff:**\nThe lower the threshold the more outliers it will remove however, we want to focus more on \"extreme outliers\" rather than just outliers. Why? because we might run the risk of information loss which will cause our models to have a lower accuracy. You can play with this threshold and see how it affects the accuracy of our classification models.\n\n### Summary:\n<ul>\n<li> <b> Visualize Distributions: </b> We first start by visualizing the distribution of the feature we are going to use to eliminate some of the outliers. V14 is the only feature that has a Gaussian distribution compared to features V12 and V10. </li>\n<li><b>Determining the threshold: </b> After we decide which number we will use to multiply with the IQR (the lower more outliers removed), we will proceed in determining the upper and lower thresholds by substrating q25 - threshold (lower extreme threshold) and adding q75 + threshold (upper extreme threshold). </li>\n<li> <b>Conditional Dropping: </b> Lastly, we create a conditional dropping stating that if the \"threshold\" is exceeded in both extremes, the instances will be removed. </li>\n<li> <b> Boxplot Representation: </b> Visualize through the boxplot that the number of \"extreme outliers\" have been reduced to a considerable amount. </li>\n</ul>\n\n**Note:** After implementing outlier reduction our accuracy has been improved by over 3%! Some outliers can distort the accuracy of our models but remember, we have to avoid an extreme amount of information loss or else our model runs the risk of underfitting.\n\n**Reference**: More information on Interquartile Range Method: <a src=\"https://machinelearningmastery.com/how-to-use-statistics-to-identify-outliers-in-data/\"> How to Use Statistics to Identify Outliers in Data </a> by Jason Brownless (Machine Learning Mastery blog)","metadata":{}},{"cell_type":"code","source":"# For fitting a gaussian distribution to our data (fit=norm)\nfrom scipy.stats import norm\n\nf, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20, 6))\n\nv14_fraud_dist = new_df['V14'].loc[new_df['Class'] == 1].values\nsns.distplot(v14_fraud_dist, ax=ax1, fit=norm, color='#FB8861')\nax1.set_title('V14 Distribution \\n (Fraud Transactions)', fontsize=14)\n\nv12_fraud_dist = new_df['V12'].loc[new_df['Class'] == 1].values\nsns.distplot(v12_fraud_dist, ax=ax2, fit=norm, color='#56F9BB')\nax2.set_title('V12 Distribution \\n (Fraud Transactions)', fontsize=14)\n\n\nv10_fraud_dist = new_df['V10'].loc[new_df['Class'] == 1].values\nsns.distplot(v10_fraud_dist,ax=ax3, fit=norm, color='#C5B3F9')\nax3.set_title('V10 Distribution \\n (Fraud Transactions)', fontsize=14)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# # -----> V14 Removing Outliers (Highest Negative Correlated with Labels)\nv14_fraud = new_df['V14'].loc[new_df['Class'] == 1].values\n# upper and lower thresholds\nq25, q75 = np.percentile(v14_fraud, 25), np.percentile(v14_fraud, 75)\nprint('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\nv14_iqr = q75 - q25\nprint('IQR: {}'.format(v14_iqr))\n# We determine the threshold by multiplying * 1.5\nv14_cut_off = v14_iqr * 1.5\nv14_lower, v14_upper = q25 - v14_cut_off, q75 + v14_cut_off\nprint('Cut Off: {}'.format(v14_cut_off))\nprint('V14 Lower: {}'.format(v14_lower))\nprint('V14 Upper: {}'.format(v14_upper))\n\noutliers = [x for x in v14_fraud if x < v14_lower or x > v14_upper]\nprint('Feature V14 Outliers for Fraud Cases: {}'.format(len(outliers)))\nprint('V10 outliers:{}'.format(outliers))\n\nnew_df = new_df.drop(new_df[(new_df['V14'] > v14_upper) | (new_df['V14'] < v14_lower)].index)\nprint('----' * 44)\n\n# -----> V12 removing outliers from fraud transactions\nv12_fraud = new_df['V12'].loc[new_df['Class'] == 1].values\nq25, q75 = np.percentile(v12_fraud, 25), np.percentile(v12_fraud, 75)\nv12_iqr = q75 - q25\n\nv12_cut_off = v12_iqr * 1.5\nv12_lower, v12_upper = q25 - v12_cut_off, q75 + v12_cut_off\nprint('V12 Lower: {}'.format(v12_lower))\nprint('V12 Upper: {}'.format(v12_upper))\noutliers = [x for x in v12_fraud if x < v12_lower or x > v12_upper]\nprint('V12 outliers: {}'.format(outliers))\nprint('Feature V12 Outliers for Fraud Cases: {}'.format(len(outliers)))\nnew_df = new_df.drop(new_df[(new_df['V12'] > v12_upper) | (new_df['V12'] < v12_lower)].index)\nprint('Number of Instances after outliers removal: {}'.format(len(new_df)))\nprint('----' * 44)\n\n# -----> V12 removing outliers from fraud transactions\nv10_fraud = new_df['V10'].loc[new_df['Class'] == 1].values\nq25, q75 = np.percentile(v10_fraud, 25), np.percentile(v10_fraud, 75)\nv10_iqr = q75 - q25\n\nv10_cut_off = v10_iqr * 1.5\nv10_lower, v10_upper = q25 - v10_cut_off, q75 + v10_cut_off\nprint('V10 Lower: {}'.format(v10_lower))\nprint('V10 Upper: {}'.format(v10_upper))\noutliers = [x for x in v10_fraud if x < v10_lower or x > v10_upper]\nprint('V10 outliers: {}'.format(outliers))\nprint('Feature V10 Outliers for Fraud Cases: {}'.format(len(outliers)))\nnew_df = new_df.drop(new_df[(new_df['V10'] > v10_upper) | (new_df['V10'] < v10_lower)].index)\nprint('Number of Instances after outliers removal: {}'.format(len(new_df)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f,(ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,6))\n\ncolors = ['#B3F9C5', '#f9c5b3']\n# Boxplots with outliers removed\n# Feature V14\nsns.boxplot(x=\"Class\", y=\"V14\", data=new_df,ax=ax1, palette=colors)\nax1.set_title(\"V14 Feature \\n Reduction of outliers\", fontsize=14)\nax1.annotate('Fewer extreme \\n outliers', xy=(0.98, -17.5), xytext=(0, -12),\n            arrowprops=dict(facecolor='black'),\n            fontsize=14)\n\n# Feature 12\nsns.boxplot(x=\"Class\", y=\"V12\", data=new_df, ax=ax2, palette=colors)\nax2.set_title(\"V12 Feature \\n Reduction of outliers\", fontsize=14)\nax2.annotate('Fewer extreme \\n outliers', xy=(0.98, -17.3), xytext=(0, -12),\n            arrowprops=dict(facecolor='black'),\n            fontsize=14)\n\n# Feature V10\nsns.boxplot(x=\"Class\", y=\"V10\", data=new_df, ax=ax3, palette=colors)\nax3.set_title(\"V10 Feature \\n Reduction of outliers\", fontsize=14)\nax3.annotate('Fewer extreme \\n outliers', xy=(0.95, -16.5), xytext=(0, -12),\n            arrowprops=dict(facecolor='black'),\n            fontsize=14)\n\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Dimensionality Reduction and Clustering: </h2>\n<a id=\"clustering\"></a>\n\n<h3>Understanding t-SNE (dimensionality reduction):  </h3>\nIn order to understand this algorithm you have to understand the following terms: <br>\n<ul>\n<li> <b> Euclidean Distance </b></li>\n<li> <b>Conditional Probability</b> </li>\n<li><b>Normal and T-Distribution Plots</b> </li>\n</ul> \n\n**Note:** If you want a simple instructive video look at <a href=\"https://www.youtube.com/watch?v=NEaUSP4YerM\"> StatQuest: t-SNE, Clearly Explained </a> by Joshua Starmer\n\n\n<h3> Summary: </h3>\n<ul> \n<li>t-SNE algorithm can pretty accurately cluster the cases that were fraud and non-fraud in our dataset. </li>\n<li> Although the subsample is pretty small, the t-SNE algorithm is able to detect clusters pretty accurately in every scenario (I shuffle the dataset before running t-SNE)</li>\n<li> This gives us an indication that further predictive models will perform pretty well in separating fraud cases from non-fraud cases. </li>\n</ul>","metadata":{}},{"cell_type":"code","source":"import time\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\n\n# New_df is from the random undersample data (fewer instances)\nX = new_df.drop('Class', axis=1)\ny = new_df['Class']\n\n# T-SNE Implementation\nt0 = time.time()\nX_reduced_tsne = TSNE(n_components=2, random_state=42).fit_transform(X.values)\nt1 = time.time()\nprint(\"T-SNE took {:.2} s\".format(t1 - t0))\n\n# PCA Implementation\nt0 = time.time()\nX_reduced_pca = PCA(n_components=2, random_state=42).fit_transform(X.values)\nt1 = time.time()\nprint(\"PCA took {:.2} s\".format(t1 - t0))\n\n# TruncatedSVD\nt0 = time.time()\nX_reduced_svd = TruncatedSVD(n_components=2, algorithm='randomized', random_state=42).fit_transform(X.values)\nt1 = time.time()\nprint(\"Truncated SVD took {:.2} s\".format(t1 - t0))","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:37:02.99486Z","iopub.execute_input":"2022-01-29T13:37:02.995264Z","iopub.status.idle":"2022-01-29T13:37:08.000815Z","shell.execute_reply.started":"2022-01-29T13:37:02.995223Z","shell.execute_reply":"2022-01-29T13:37:07.999309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import matplotlib.patches as mpatches\n\nf, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24,6))\n# labels = ['No Fraud', 'Fraud']\nf.suptitle('Clusters using Dimensionality Reduction', fontsize=14)\n\nblue_patch = mpatches.Patch(color='#0A0AFF', label='No Fraud')\nred_patch = mpatches.Patch(color='#AF0000', label='Fraud')\n\n# t-SNE scatter plot\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nax1.set_title('t-SNE', fontsize=14)\n\nax1.grid(True)\n\nax1.legend(handles=[blue_patch, red_patch])\n\n# PCA scatter plot\nax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nax2.set_title('PCA', fontsize=14)\n\nax2.grid(True)\n\nax2.legend(handles=[blue_patch, red_patch])\n\n# TruncatedSVD scatter plot\nax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nax3.set_title('Truncated SVD', fontsize=14)\n\nax3.grid(True)\n\nax3.legend(handles=[blue_patch, red_patch])\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:37:18.494407Z","iopub.execute_input":"2022-01-29T13:37:18.494771Z","iopub.status.idle":"2022-01-29T13:37:19.334313Z","shell.execute_reply.started":"2022-01-29T13:37:18.494728Z","shell.execute_reply":"2022-01-29T13:37:19.333626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2> Classifiers (UnderSampling):  </h2>\n<a id=\"classifiers\"></a>\nIn this section we will train four types of classifiers and decide which classifier will be more effective in detecting <b>fraud transactions</b>.  Before we have to split our data into training and testing sets and separate the features from the labels.\n\n## Summary: \n<ul>\n<li> <b> Logistic Regression </b> classifier is more accurate than the other three classifiers in most cases. (We will further analyze Logistic Regression) </li>\n<li><b> GridSearchCV </b> is used to determine the paremeters that gives the best predictive score for the classifiers. </li>\n<li> Logistic Regression has the best Receiving Operating Characteristic score  (ROC), meaning that LogisticRegression pretty accurately separates <b> fraud </b> and <b> non-fraud </b> transactions.</li>\n</ul>\n\n## Learning Curves:\n<ul>\n<li>The <b>wider the  gap</b>  between the training score and the cross validation score, the more likely your model is <b>overfitting (high variance)</b>.</li>\n<li> If the score is low in both training and cross-validation sets</b> this is an indication that our model is <b>underfitting (high bias)</b></li>\n<li><b> Logistic Regression Classifier</b>  shows the best score in both training and cross-validating sets.</li>\n</ul>","metadata":{}},{"cell_type":"code","source":"# Undersampling before cross validating (prone to overfit)\nX = new_df.drop('Class', axis=1)\ny = new_df['Class']","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:37:50.576972Z","iopub.execute_input":"2022-01-29T13:37:50.577609Z","iopub.status.idle":"2022-01-29T13:37:50.583319Z","shell.execute_reply.started":"2022-01-29T13:37:50.577575Z","shell.execute_reply":"2022-01-29T13:37:50.581968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Our data is already scaled we should split our training and test sets\nfrom sklearn.model_selection import train_test_split\n\n# This is explicitly used for undersampling.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:37:56.530772Z","iopub.execute_input":"2022-01-29T13:37:56.531689Z","iopub.status.idle":"2022-01-29T13:37:56.538973Z","shell.execute_reply.started":"2022-01-29T13:37:56.531647Z","shell.execute_reply":"2022-01-29T13:37:56.538161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Turn the values into an array for feeding the classification algorithms.\nX_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:38:00.049885Z","iopub.execute_input":"2022-01-29T13:38:00.050522Z","iopub.status.idle":"2022-01-29T13:38:00.055741Z","shell.execute_reply.started":"2022-01-29T13:38:00.050481Z","shell.execute_reply":"2022-01-29T13:38:00.054971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's implement simple classifiers\n# Classifier Libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport collections\n\nclassifiers = {\n    \"LogisiticRegression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(),\n    \"Support Vector Classifier\": SVC(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n}","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:38:07.736259Z","iopub.execute_input":"2022-01-29T13:38:07.737152Z","iopub.status.idle":"2022-01-29T13:38:07.769654Z","shell.execute_reply.started":"2022-01-29T13:38:07.737106Z","shell.execute_reply":"2022-01-29T13:38:07.768919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Wow our scores are getting even high scores even when applying cross validation.\nfrom sklearn.model_selection import cross_val_score\n\nfor key, classifier in classifiers.items():\n    classifier.fit(X_train, y_train)\n    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:38:25.197869Z","iopub.execute_input":"2022-01-29T13:38:25.198332Z","iopub.status.idle":"2022-01-29T13:38:25.707284Z","shell.execute_reply.started":"2022-01-29T13:38:25.198298Z","shell.execute_reply":"2022-01-29T13:38:25.706163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use GridSearchCV to find the best parameters.\nfrom sklearn.model_selection import GridSearchCV\n# Other Libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\nfrom collections import Counter\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Logistic Regression \nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\ngrid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\ngrid_log_reg.fit(X_train, y_train)\n# We automatically get the logistic regression with the best parameters.\nlog_reg = grid_log_reg.best_estimator_\n\nknears_params = {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n\ngrid_knears = GridSearchCV(KNeighborsClassifier(), knears_params)\ngrid_knears.fit(X_train, y_train)\n# KNears best estimator\nknears_neighbors = grid_knears.best_estimator_\n\n# Support Vector Classifier\nsvc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\ngrid_svc = GridSearchCV(SVC(), svc_params)\ngrid_svc.fit(X_train, y_train)\n\n# SVC best estimator\nsvc = grid_svc.best_estimator_\n\n# DecisionTree Classifier\ntree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \n              \"min_samples_leaf\": list(range(5,7,1))}\ngrid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\ngrid_tree.fit(X_train, y_train)\n\n# tree best estimator\ntree_clf = grid_tree.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:38:45.640348Z","iopub.execute_input":"2022-01-29T13:38:45.64119Z","iopub.status.idle":"2022-01-29T13:38:49.273352Z","shell.execute_reply.started":"2022-01-29T13:38:45.641138Z","shell.execute_reply":"2022-01-29T13:38:49.272431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Overfitting Case\n\nlog_reg_score = cross_val_score(log_reg, X_train, y_train, cv=5)\nprint('Logistic Regression Cross Validation Score: ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')\n\nknears_score = cross_val_score(knears_neighbors, X_train, y_train, cv=5)\nprint('Knears Neighbors Cross Validation Score', round(knears_score.mean() * 100, 2).astype(str) + '%')\n\nsvc_score = cross_val_score(svc, X_train, y_train, cv=5)\nprint('Support Vector Classifier Cross Validation Score', round(svc_score.mean() * 100, 2).astype(str) + '%')\n\ntree_score = cross_val_score(tree_clf, X_train, y_train, cv=5)\nprint('DecisionTree Classifier Cross Validation Score', round(tree_score.mean() * 100, 2).astype(str) + '%')","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:38:59.123641Z","iopub.execute_input":"2022-01-29T13:38:59.123936Z","iopub.status.idle":"2022-01-29T13:38:59.471439Z","shell.execute_reply.started":"2022-01-29T13:38:59.123907Z","shell.execute_reply":"2022-01-29T13:38:59.470769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We will undersample during cross validating\nundersample_X = df.drop('Class', axis=1)\nundersample_y = df['Class']\n\nfor train_index, test_index in skf.split(undersample_X, undersample_y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    undersample_Xtrain, undersample_Xtest = undersample_X.iloc[train_index], undersample_X.iloc[test_index]\n    undersample_ytrain, undersample_ytest = undersample_y.iloc[train_index], undersample_y.iloc[test_index]\n    \nundersample_Xtrain = undersample_Xtrain.values\nundersample_Xtest = undersample_Xtest.values\nundersample_ytrain = undersample_ytrain.values\nundersample_ytest = undersample_ytest.values \n\nundersample_accuracy = []\nundersample_precision = []\nundersample_recall = []\nundersample_f1 = []\nundersample_auc = []\n\n# Implementing NearMiss Technique \n# Distribution of NearMiss (Just to see how it distributes the labels we won't use these variables)\nnm = NearMiss()\nX_nearmiss, y_nearmiss = nm.fit_resample(undersample_X.values, undersample_y.values)\nprint('NearMiss Label Distribution: {}'.format(Counter(y_nearmiss)))\n# Cross Validating the right way\n\nfor train, test in skf.split(undersample_Xtrain, undersample_ytrain):\n    undersample_pipeline = imbalanced_make_pipeline(NearMiss(sampling_strategy='majority'), log_reg) # SMOTE happens during Cross Validation not before..\n    undersample_model = undersample_pipeline.fit(undersample_Xtrain[train], undersample_ytrain[train])\n    undersample_prediction = undersample_model.predict(undersample_Xtrain[test])    \n    undersample_accuracy.append(undersample_pipeline.score(original_Xtrain[test], original_ytrain[test]))\n    undersample_precision.append(precision_score(original_ytrain[test], undersample_prediction))\n    undersample_recall.append(recall_score(original_ytrain[test], undersample_prediction))\n    undersample_f1.append(f1_score(original_ytrain[test], undersample_prediction))\n    undersample_auc.append(roc_auc_score(original_ytrain[test], undersample_prediction))","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:39:12.695936Z","iopub.execute_input":"2022-01-29T13:39:12.696907Z","iopub.status.idle":"2022-01-29T13:39:35.912496Z","shell.execute_reply.started":"2022-01-29T13:39:12.696837Z","shell.execute_reply":"2022-01-29T13:39:35.91154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's Plot LogisticRegression Learning Curve\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import learning_curve\n\ndef plot_learning_curve(estimator1, estimator2, estimator3, estimator4, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(20,14), sharey=True)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    # First Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator1, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax1.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax1.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax1.set_title(\"Logistic Regression Learning Curve\", fontsize=14)\n    ax1.set_xlabel('Training size (m)')\n    ax1.set_ylabel('Score')\n    ax1.grid(True)\n    ax1.legend(loc=\"best\")\n    \n    # Second Estimator \n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator2, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax2.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax2.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax2.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax2.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax2.set_title(\"Knears Neighbors Learning Curve\", fontsize=14)\n    ax2.set_xlabel('Training size (m)')\n    ax2.set_ylabel('Score')\n    ax2.grid(True)\n    ax2.legend(loc=\"best\")\n    \n    # Third Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator3, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax3.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax3.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax3.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax3.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax3.set_title(\"Support Vector Classifier \\n Learning Curve\", fontsize=14)\n    ax3.set_xlabel('Training size (m)')\n    ax3.set_ylabel('Score')\n    ax3.grid(True)\n    ax3.legend(loc=\"best\")\n    \n    # Fourth Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator4, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax4.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax4.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax4.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax4.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax4.set_title(\"Decision Tree Classifier \\n Learning Curve\", fontsize=14)\n    ax4.set_xlabel('Training size (m)')\n    ax4.set_ylabel('Score')\n    ax4.grid(True)\n    ax4.legend(loc=\"best\")\n    return plt","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:39:35.914856Z","iopub.execute_input":"2022-01-29T13:39:35.915408Z","iopub.status.idle":"2022-01-29T13:39:35.951425Z","shell.execute_reply.started":"2022-01-29T13:39:35.915359Z","shell.execute_reply":"2022-01-29T13:39:35.9503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\nplot_learning_curve(log_reg, knears_neighbors, svc, tree_clf, X_train, y_train, (0.87, 1.01), cv=cv, n_jobs=4)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:39:41.609306Z","iopub.execute_input":"2022-01-29T13:39:41.609942Z","iopub.status.idle":"2022-01-29T13:39:57.158136Z","shell.execute_reply.started":"2022-01-29T13:39:41.609885Z","shell.execute_reply":"2022-01-29T13:39:57.157235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve\nfrom sklearn.model_selection import cross_val_predict\n# Create a DataFrame with all the scores and the classifiers names.\n\nlog_reg_pred = cross_val_predict(log_reg, X_train, y_train, cv=5,\n                             method=\"decision_function\")\n\nknears_pred = cross_val_predict(knears_neighbors, X_train, y_train, cv=5)\n\nsvc_pred = cross_val_predict(svc, X_train, y_train, cv=5,\n                             method=\"decision_function\")\n\ntree_pred = cross_val_predict(tree_clf, X_train, y_train, cv=5)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:40:15.07695Z","iopub.execute_input":"2022-01-29T13:40:15.077235Z","iopub.status.idle":"2022-01-29T13:40:15.419688Z","shell.execute_reply.started":"2022-01-29T13:40:15.077206Z","shell.execute_reply":"2022-01-29T13:40:15.418767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\nprint('Logistic Regression: ', roc_auc_score(y_train, log_reg_pred))\nprint('KNears Neighbors: ', roc_auc_score(y_train, knears_pred))\nprint('Support Vector Classifier: ', roc_auc_score(y_train, svc_pred))\nprint('Decision Tree Classifier: ', roc_auc_score(y_train, tree_pred))","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:40:18.778877Z","iopub.execute_input":"2022-01-29T13:40:18.779172Z","iopub.status.idle":"2022-01-29T13:40:18.793854Z","shell.execute_reply.started":"2022-01-29T13:40:18.779141Z","shell.execute_reply":"2022-01-29T13:40:18.793162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_fpr, log_tpr, log_thresold = roc_curve(y_train, log_reg_pred)\nknear_fpr, knear_tpr, knear_threshold = roc_curve(y_train, knears_pred)\nsvc_fpr, svc_tpr, svc_threshold = roc_curve(y_train, svc_pred)\ntree_fpr, tree_tpr, tree_threshold = roc_curve(y_train, tree_pred)\n\ndef graph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr):\n    plt.figure(figsize=(16,8))\n    plt.title('ROC Curve \\n Top 4 Classifiers', fontsize=18)\n    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_train, log_reg_pred)))\n    plt.plot(knear_fpr, knear_tpr, label='KNears Neighbors Classifier Score: {:.4f}'.format(roc_auc_score(y_train, knears_pred)))\n    plt.plot(svc_fpr, svc_tpr, label='Support Vector Classifier Score: {:.4f}'.format(roc_auc_score(y_train, svc_pred)))\n    plt.plot(tree_fpr, tree_tpr, label='Decision Tree Classifier Score: {:.4f}'.format(roc_auc_score(y_train, tree_pred)))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.01, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n                )\n    plt.legend()\n    \ngraph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:40:28.338883Z","iopub.execute_input":"2022-01-29T13:40:28.339162Z","iopub.status.idle":"2022-01-29T13:40:28.695768Z","shell.execute_reply.started":"2022-01-29T13:40:28.33913Z","shell.execute_reply":"2022-01-29T13:40:28.694517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## A Deeper Look into LogisticRegression:\n<a id=\"logistic\"></a>\nIn this section we will ive a deeper look into the <b> logistic regression classifier</b>.\n\n\n### Terms:\n<ul>\n<li><b>True Positives:</b> Correctly Classified Fraud Transactions </li>\n<li><b>False Positives:</b> Incorrectly Classified Fraud Transactions</li>\n<li> <b>True Negative:</b> Correctly Classified Non-Fraud Transactions</li>\n<li> <b>False Negative:</b> Incorrectly Classified Non-Fraud Transactions</li>\n<li><b>Precision: </b>  True Positives/(True Positives + False Positives)  </li>\n<li><b> Recall: </b> True Positives/(True Positives + False Negatives)   </li>\n<li> Precision as the name says, says how precise (how sure) is our model in detecting fraud transactions while recall is the amount of fraud cases our model is able to detect.</li>\n<li><b>Precision/Recall Tradeoff: </b> The more precise (selective) our model is, the less cases it will detect. Example: Assuming that our model has a precision of 95%, Let's say there are only 5 fraud cases in which the model is 95% precise or more that these are fraud cases. Then let's say there are 5 more cases that our model considers 90% to be a fraud case, if we lower the precision there are more cases that our model will be able to detect. </li>\n</ul>\n\n### Summary:\n<ul>\n<li> <b>Precision starts to descend</b> between 0.90 and 0.92 nevertheless, our precision score is still pretty high and still we have a descent recall score. </li>\n\n</ul>","metadata":{}},{"cell_type":"code","source":"def logistic_roc_curve(log_fpr, log_tpr):\n    plt.figure(figsize=(12,8))\n    plt.title('Logistic Regression ROC Curve', fontsize=16)\n    plt.plot(log_fpr, log_tpr, 'b-', linewidth=2)\n    plt.plot([0, 1], [0, 1], 'r--')\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.axis([-0.01,1,0,1])\n        \nlogistic_roc_curve(log_fpr, log_tpr)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:41:31.73862Z","iopub.execute_input":"2022-01-29T13:41:31.740024Z","iopub.status.idle":"2022-01-29T13:41:32.019545Z","shell.execute_reply.started":"2022-01-29T13:41:31.739943Z","shell.execute_reply":"2022-01-29T13:41:32.018266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\n\nprecision, recall, threshold = precision_recall_curve(y_train, log_reg_pred)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:41:35.230101Z","iopub.execute_input":"2022-01-29T13:41:35.230377Z","iopub.status.idle":"2022-01-29T13:41:35.236544Z","shell.execute_reply.started":"2022-01-29T13:41:35.230346Z","shell.execute_reply":"2022-01-29T13:41:35.235577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\ny_pred = log_reg.predict(X_train)\n\n# Overfitting Case\nprint('---' * 45)\nprint('Overfitting: \\n')\nprint('Recall Score: {:.2f}'.format(recall_score(y_train, y_pred)))\nprint('Precision Score: {:.2f}'.format(precision_score(y_train, y_pred)))\nprint('F1 Score: {:.2f}'.format(f1_score(y_train, y_pred)))\nprint('Accuracy Score: {:.2f}'.format(accuracy_score(y_train, y_pred)))\nprint('---' * 45)\n\n# How it should look like\nprint('---' * 45)\nprint('How it should be:\\n')\nprint(\"Accuracy Score: {:.2f}\".format(np.mean(undersample_accuracy)))\nprint(\"Precision Score: {:.2f}\".format(np.mean(undersample_precision)))\nprint(\"Recall Score: {:.2f}\".format(np.mean(undersample_recall)))\nprint(\"F1 Score: {:.2f}\".format(np.mean(undersample_f1)))\nprint('---' * 45)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:41:45.141554Z","iopub.execute_input":"2022-01-29T13:41:45.142661Z","iopub.status.idle":"2022-01-29T13:41:45.169201Z","shell.execute_reply.started":"2022-01-29T13:41:45.142581Z","shell.execute_reply":"2022-01-29T13:41:45.168211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"undersample_y_score = log_reg.decision_function(original_Xtest)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:41:49.167717Z","iopub.execute_input":"2022-01-29T13:41:49.168052Z","iopub.status.idle":"2022-01-29T13:41:49.178459Z","shell.execute_reply.started":"2022-01-29T13:41:49.168019Z","shell.execute_reply":"2022-01-29T13:41:49.17748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import average_precision_score\n\nundersample_average_precision = average_precision_score(original_ytest, undersample_y_score)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      undersample_average_precision))","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:41:51.87722Z","iopub.execute_input":"2022-01-29T13:41:51.877572Z","iopub.status.idle":"2022-01-29T13:41:51.899816Z","shell.execute_reply.started":"2022-01-29T13:41:51.877539Z","shell.execute_reply":"2022-01-29T13:41:51.899076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(12,6))\n\nprecision, recall, _ = precision_recall_curve(original_ytest, undersample_y_score)\n\nplt.step(recall, precision, color='#004a93', alpha=0.2, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.2, color='#48a6ff')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('UnderSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n          undersample_average_precision), fontsize=16)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:41:55.374655Z","iopub.execute_input":"2022-01-29T13:41:55.37517Z","iopub.status.idle":"2022-01-29T13:41:55.747495Z","shell.execute_reply.started":"2022-01-29T13:41:55.375137Z","shell.execute_reply":"2022-01-29T13:41:55.746638Z"},"trusted":true},"execution_count":null,"outputs":[]}]}