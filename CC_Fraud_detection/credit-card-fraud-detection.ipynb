{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# https://www.kaggle.com/joparga3/in-depth-skewed-data-classif-93-recall-acc-now/notebook\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-17T20:59:49.116395Z","iopub.execute_input":"2022-01-17T20:59:49.117229Z","iopub.status.idle":"2022-01-17T20:59:49.127933Z","shell.execute_reply.started":"2022-01-17T20:59:49.117182Z","shell.execute_reply":"2022-01-17T20:59:49.126949Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"**Why do I use this kernel?**\n\nI have a deeply interest in find out if a transaction is a fraud or not. \nAbout the Data set: \n*     it contains transactions made by credit cards in September 2013 by european cardholders.\n*     It's highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n*     It contains only numerical input variables which are the result of a PCA transformation (confidential data).","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-01-17T20:59:49.134374Z","iopub.execute_input":"2022-01-17T20:59:49.135181Z","iopub.status.idle":"2022-01-17T20:59:49.143890Z","shell.execute_reply.started":"2022-01-17T20:59:49.135140Z","shell.execute_reply":"2022-01-17T20:59:49.143017Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Read data\ndf = pd.read_csv('/kaggle/input/creditcardfraud/creditcard.csv')","metadata":{"execution":{"iopub.status.busy":"2022-01-17T20:59:49.145938Z","iopub.execute_input":"2022-01-17T20:59:49.146464Z","iopub.status.idle":"2022-01-17T20:59:51.769567Z","shell.execute_reply.started":"2022-01-17T20:59:49.146431Z","shell.execute_reply":"2022-01-17T20:59:51.768926Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Check the columns and general structure\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T20:59:51.770900Z","iopub.execute_input":"2022-01-17T20:59:51.771319Z","iopub.status.idle":"2022-01-17T20:59:51.796523Z","shell.execute_reply.started":"2022-01-17T20:59:51.771289Z","shell.execute_reply":"2022-01-17T20:59:51.795665Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## As we can see, the data is confidential, we don't know the column names.","metadata":{}},{"cell_type":"code","source":"# Search missing values\ndf.isnull().sum().max()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T20:59:51.797751Z","iopub.execute_input":"2022-01-17T20:59:51.797976Z","iopub.status.idle":"2022-01-17T20:59:51.823277Z","shell.execute_reply.started":"2022-01-17T20:59:51.797947Z","shell.execute_reply":"2022-01-17T20:59:51.822228Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## No missing values","metadata":{}},{"cell_type":"markdown","source":"# See if the data is balanced or unbalanced","metadata":{}},{"cell_type":"code","source":"from matplotlib.legend_handler import HandlerBase\nfrom matplotlib.text import Text\nclass TextHandler(HandlerBase):\n    def create_artists(self, legend, tup ,xdescent, ydescent, width, height, fontsize,trans):\n        tx = Text(width/2.,height/2,tup[0], fontsize=fontsize,\n                  ha=\"center\", va=\"center\", color=tup[1], fontweight=\"bold\")\n        return [tx]\n    \nax = sns.countplot(x=\"Class\", data=df)\nplt.title(\"Frauds vs Not. Frauds\")\nplt.xlabel(\"Target\")\nplt.ylabel(\"Count\")\nsns.set(rc = {'figure.figsize':(8,8)})\n#Add value counts to each bar\nfor p in ax.patches:\n   ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.25, p.get_height()+0.01))\n\n# Add right legend\nhandltext = [\"0\", \"1\"]\nlabels = [\"Not Fraud\", \"Fraud\"]\n\nt = ax.get_xticklabels()\nlabeldic = dict(zip(handltext, labels))\nlabels = [labeldic[h.get_text()]  for h in t]\nhandles = [(h.get_text(),c.get_fc()) for h,c in zip(t,ax.patches)]\n\nax.legend(handles, labels, handler_map={tuple : TextHandler()}) \n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-17T20:59:51.825718Z","iopub.execute_input":"2022-01-17T20:59:51.826019Z","iopub.status.idle":"2022-01-17T20:59:52.090336Z","shell.execute_reply.started":"2022-01-17T20:59:51.825986Z","shell.execute_reply":"2022-01-17T20:59:52.089348Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# The classes are heavily skewed we need to solve this issue later.\nprint('No Frauds', round(df['Class'].value_counts()[0]/len(df) * 100,2), '% of the dataset')\nprint('Frauds', round(df['Class'].value_counts()[1]/len(df) * 100,2), '% of the dataset')","metadata":{"execution":{"iopub.status.busy":"2022-01-17T20:59:52.091537Z","iopub.execute_input":"2022-01-17T20:59:52.091783Z","iopub.status.idle":"2022-01-17T20:59:52.103936Z","shell.execute_reply.started":"2022-01-17T20:59:52.091751Z","shell.execute_reply":"2022-01-17T20:59:52.103016Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"**Distributions:** By seeing the distributions we can have an idea how skewed are these features, we can also see further distributions of the other features. ","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(18,4))\n\namount_val = df['Amount'].values\ntime_val = df['Time'].values\n\nsns.distplot(amount_val, ax=ax[0], color='r')\nax[0].set_title('Distribution of Transaction Amount', fontsize=14)\nax[0].set_xlim([min(amount_val), max(amount_val)])\n\nsns.distplot(time_val, ax=ax[1], color='b')\nax[1].set_title('Distribution of Transaction Time', fontsize=14)\nax[1].set_xlim([min(time_val), max(time_val)])","metadata":{"execution":{"iopub.status.busy":"2022-01-17T21:01:51.206121Z","iopub.execute_input":"2022-01-17T21:01:51.206435Z","iopub.status.idle":"2022-01-17T21:01:54.237575Z","shell.execute_reply.started":"2022-01-17T21:01:51.206394Z","shell.execute_reply":"2022-01-17T21:01:54.236969Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Unbalance Data\n\n## What can we do?\n\n* Collect more data.\n* Use a correct metric (don't use accuracy):\n    * Use the confusio nmatrix to calculate Precision, Recall\n    * F1score (weighted average of precision recall)\n    * Use Kappa - which is a classification accuracy normalized by the imbalance of the classes in the data\n    * ROC curves - calculates sensitivity/specificity ratio.\n* Resampling the dataset (process the data to have an approximate 50-50 ratio).\n    * OVER-sampling, adding copies of the under-represented class (better when you have little data).\n    * UNDER-sampling, deletes instances from the over-represented class (better when he have lot's of data).\n","metadata":{}},{"cell_type":"markdown","source":"## Scaling and Distributing\nIt's necesary to scale columns Time and Amount. The other columns are already scaled. ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, RobustScaler\n\n# IMPORTANT: RobustScaler is less prone to outliers.\n\nstd_scaler = StandardScaler()\nrob_scaler = RobustScaler()\n\ndf['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n\n# Erase old not scaled columns Time and Amount\ndf.drop(['Time','Amount'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T21:10:41.414364Z","iopub.execute_input":"2022-01-17T21:10:41.415010Z","iopub.status.idle":"2022-01-17T21:10:41.674990Z","shell.execute_reply.started":"2022-01-17T21:10:41.414960Z","shell.execute_reply":"2022-01-17T21:10:41.674003Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"scaled_amount = df['scaled_amount']\nscaled_time = df['scaled_time']\n\ndf.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\ndf.insert(0, 'scaled_amount', scaled_amount)\ndf.insert(1, 'scaled_time', scaled_time)\n\n# Amount and Time are Scaled!\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T21:11:58.482794Z","iopub.execute_input":"2022-01-17T21:11:58.483093Z","iopub.status.idle":"2022-01-17T21:11:58.543240Z","shell.execute_reply.started":"2022-01-17T21:11:58.483062Z","shell.execute_reply":"2022-01-17T21:11:58.542653Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Sub-Sample the data\n\nIt will be a dataframe with a 50/50 ratio of fraud and non-fraud transactions. \n\n## Why do we create a sub-Sample?\n\nUsing the original dataframe will cause the following issues:\n\n* **Overfitting:** Our classification models will assume that in most cases there are no frauds! What we want for our model is to be certain when a fraud occurs.\n* **Wrong Correlations:** Although we don't know what the \"V\" features stand for, it will be useful to understand how each of this features influence the result (Fraud or No Fraud) by having an imbalance dataframe we are not able to see the true correlations between the class and features.","metadata":{}},{"cell_type":"markdown","source":"# Splitting the Data (Original DataFrame)\n\nBefore proceeding with the Random UnderSampling technique we have to separate the orginal dataframe. Why? for testing purposes, remember although we are splitting the data when implementing Random UnderSampling or OverSampling techniques, we want to test our models on the original testing set not on the testing set created by either of these techniques. The main goal is to fit the model either with the dataframes that were undersample and oversample (in order for our models to detect the patterns), and test it on the original testing set.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\nprint('No Frauds', round(df['Class'].value_counts()[0]/len(df) * 100,2), '% of the dataset')\nprint('Frauds', round(df['Class'].value_counts()[1]/len(df) * 100,2), '% of the dataset')\n\nX = df.drop('Class', axis=1)\ny = df['Class']\n\nsss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n\nfor train_index, test_index in sss.split(X, y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n\n# We already have X_train and y_train for undersample data thats why I am using original to distinguish and to not overwrite these variables.\n# original_Xtrain, original_Xtest, original_ytrain, original_ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Check the Distribution of the labels\n\n\n# Turn into an array\noriginal_Xtrain = original_Xtrain.values\noriginal_Xtest = original_Xtest.values\noriginal_ytrain = original_ytrain.values\noriginal_ytest = original_ytest.values\n\n# See if both the train and test label distribution are similarly distributed\ntrain_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\ntest_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\nprint('-' * 100)\n\nprint('Label Distributions: \\n')\nprint(train_counts_label/ len(original_ytrain))\nprint(test_counts_label/ len(original_ytest))","metadata":{"execution":{"iopub.status.busy":"2022-01-17T21:17:47.791701Z","iopub.execute_input":"2022-01-17T21:17:47.792335Z","iopub.status.idle":"2022-01-17T21:17:48.040496Z","shell.execute_reply.started":"2022-01-17T21:17:47.792301Z","shell.execute_reply":"2022-01-17T21:17:48.039509Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}